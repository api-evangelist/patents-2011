---

title: Context aware augmentation interactions
abstract: A mobile platform renders different augmented reality objects based on the spatial relationship, such as the proximity and/or relative positions between real-world objects. The mobile platform detects and tracks a first object and a second object in one or more captured images. The mobile platform determines the spatial relationship of the objects, e.g., the proximity or distance between objects and/or the relative positions between objects. The proximity may be based on whether the objects appear in the same image or the distance between the objects. Based on the spatial relationship of the objects, the augmentation object to be rendered is determined, e.g., by searching a database. The selected augmentation object is rendered and displayed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08509483&OS=08509483&RS=08509483
owner: QUALCOMM Incorporated
number: 08509483
owner_city: San Diego
owner_country: US
publication_date: 20110131
---
In augmented reality AR applications a real world object is imaged and displayed on a screen along with computer generated information such as an image or textual information. AR can be used to provide information either graphical or textual about a real world object such as a building or product. Typically the AR object that is rendered is dependent on the real world object that is imaged. However the context of that real world object e.g. the location or other surrounding objects is not considered when rendering an AR object. It is desirable however to be able to display AR content that has context to the physical surroundings or proximity to other products.

A mobile platform renders different augmented reality objects based on the spatial relationship such as the proximity and or relative positions between real world objects. The spatial relationship which may be proximity of the objects or the relative positions of the objects provides the context of the real world objects. The mobile platform detects and tracks a first object and a second object in one or more captured images. The mobile platform then determines the spatial relationship of the objects as the proximity or distance and or the relative positions between objects. The proximity may be based on whether the objects appear in the same image or a quantitative distance between the objects determined e.g. based on pose information or through image processing. Based on the spatial relationship of the objects the augmentation object to be rendered is determined e.g. by searching a database. The selected augmentation object is rendered and displayed.

The AR object rendered and displayed by mobile platform is dependent on not only on the real world objects that are imaged but the spatial relationships of the imaged objects. By way of example the spatial relationships may be the proximity of other imaged objects. Thus one type of AR object may be rendered when a real world object is near another complementary real world object while a different AR object or different behavior of the AR object may be rendered when the objects are not near or when an incompatible real world object is near. For example one type of AR object may be rendered when breakfast products are placed near each other while a different AR object or different behavior of the AR object may be rendered when a breakfast product is located near a liquor bottle. The spatial relationships of the real world object may include other factors such the relative positions of the objects as well as other factors such as the time of day or geographic location.

A second object in an image captured by the camera is also detected and tracked . Again once detected the second object may also be identified e.g. by consulting an on board database or by accessing a remote server. By way of example the object B imaged by the mobile platform in is detected and tracked. The second object may be an item that is located near and in the same frame of view i.e. in the same captured image as the first object. Additionally the second object may be an item that is in a separate frame of view i.e. the second object appears in a different captured image than the first object. Thus for example the first object may be imaged detected and tracked. The mobile platform may then be moved to capture an image of the second object which is detected and tracked.

The spatial relationship of the objects is then determined to provide the context for the objects. If desired the spatial relationship of the object may be determined only if the objects are identified as objects that have associated contextually dependent augmented reality objects e.g. which may be determined by accessing an on board or external database. The spatial relationships may be the proximity of the imaged objects. The first object and the second object may be considered to be proximate if at least portions of both objects are in the field of view of the camera at the same time. Alternatively proximity may be determined as a quantitative distance between the first object and the second object. For example the distance may be determined by comparing the distance between the objects to the size of the objects or using the pose information from tracking the first object and the second object. The distance may be compared to one or more thresholds to determine whether the objects are considered proximate. Additionally if the first object and second object do not appear in the same image the proximity of the objects may be determined based on the distance between the objects as determined using tracking information and data from the motion sensors in the mobile platform . The spatial relationships may also or alternatively be the relative positions of the first object and the second object e.g. whether the first object is in front of or above the second object. Additional contextual information may also be used such as the presence or absence of one or more additional real world objects the time of day the ambient light the geographic location of the mobile platform etc.

Using the determined spatial relationships between the identified first object and the identified second object the AR object to be rendered may be determined . For example a database of AR objects may be maintained for different spatial relationships of specific real world objects. Once the spatial relationship of the real world objects is determined the database may be accessed to determine the AR object to be rendered. The AR object may then be rendered and displayed by the mobile platform .

By way of illustration shows an AR object that is rendered and displayed based on the spatial relationship of identified object A and identified object B . The AR object is in the form of an arrow to indicate e.g. that object A is compatible with object B . on the other hand illustrates mobile platform producing an image similar to that shown in except a third object is identified object C is detected and tracked and determined to be proximate to object B . A different AR object which includes an arrow with a line through it is rendered based on the spatial relationship e.g. proximity of object B and incompatible object C . If desired the AR object may be animated and the behavior of the animated AR object may change based on the spatial relationship of the real world objects. Additionally if desired additional real world objects may be detected and tracked and used to determine the AR object to be rendered.

The mobile platform also includes a control unit that is connected to and communicates with the camera motion sensors and user interface . The control unit accepts and processes data from the camera and motion sensors and controls the display in response. The control unit may be provided by a processor and associated memory hardware software and firmware . The control unit may include an image processor for processing the images from the camera to detect real world objects. The control unit may also include a position processor to determine and track the pose of the mobile platform with respect to the real world objects e.g. based on data received form the motion sensors and or based on vision based tracking techniques using additional images captured by the camera . The position processor may also determine the proximity of real world objects as well as the spatial relations of the objects. The control unit may further include a graphics engine which may be e.g. a gaming engine to render desired AR objects with respect to the location and spatial relationship of the real world objects. The graphics engine may retrieve AR objects from a database which may be in memory based on the spatial relationship of the real world objects such as the proximity and or relative positions of the objects as well as any other desired factors such as time of day which may be determined based on clock and geographic location which may be determined based on data from an optional satellite position system SPS receiver . The image processor position processor and graphics engine are illustrated separately from processor for clarity but may be part of the processor or implemented in the processor based on instructions in the software which is run in the processor . It will be understood as used herein that the processor can but need not necessarily include one or more microprocessors embedded processors controllers application specific integrated circuits ASICs digital signal processors DSPs and the like. The term processor is intended to describe the functions implemented by the system rather than specific hardware. Moreover as used herein the term memory refers to any type of computer storage medium including long term short term or other memory associated with the mobile platform and is not to be limited to any particular type of memory or number of memories or type of media upon which memory is stored.

The device includes means for means for detecting and tracking the first object and the second object in one or more images which may include the image processor position processor as well as the motion sensors if desired. The device further includes a means for means for determining a spatial relationship between the first object and the second object which may include the position processor . A means for determining an augmentation object to render based on the spatial relationship between the first object and the second object may include the graphics engine which accesses a database . Additionally the device may include a means for determining a pose position and orientation of the mobile platform associated with the different objects which may include the position processor as well as motion sensors . The device may include a means for comparing the size of the objects to the space between the objects to determine the distance between the objects which may be the image processor .

The methodologies described herein may be implemented by various means depending upon the application. For example these methodologies may be implemented in hardware firmware software or any combination thereof For a hardware implementation the processing units may be implemented within one or more application specific integrated circuits ASICs digital signal processors DSPs digital signal processing devices DSPDs programmable logic devices PLDs field programmable gate arrays FPGAs processors controllers micro controllers microprocessors electronic devices other electronic units designed to perform the functions described herein or a combination thereof.

For a firmware and or software implementation the methodologies may be implemented with modules e.g. procedures functions and so on that perform the functions described herein. Any machine readable medium tangibly embodying instructions may be used in implementing the methodologies described herein. For example software codes may be stored in memory and executed by the processor . Memory may be implemented within or external to the processor .

If implemented in firmware and or software the functions may be stored as one or more instructions or code on a computer readable medium. Examples include non transitory computer readable media encoded with a data structure and computer readable media encoded with a computer program. For example the computer readable medium including program code stored thereon may include program code to display on the display an image of a first object and a second object program code to detect and track the first object and to detect and track the second object program code to determine a spatial relationship between the first object and the second object program code to determine an augmentation object to render based on the spatial relationship between the first object and the second object and program code to display on the display the augmentation object. The computer readable medium may further include program code to determine a first pose with respect to the first object and a second pose with respect to the second object wherein the program code to determine the distance between the first object and the second object uses the first pose and the second pose to determine the distance. The computer readable medium may further include program code to compare the size of at least one of the first object and the second object to a space between the first object and the second object to determine the distance between the first object and the second object. Computer readable media includes physical computer storage media. A storage medium may be any available medium that can be accessed by a computer. By way of example and not limitation such computer readable media can comprise RAM ROM EEPROM CD ROM or other optical disk storage magnetic disk storage or other magnetic storage devices or any other medium that can be used to store desired program code in the form of instructions or data structures and that can be accessed by a computer disk and disc as used herein includes compact disc CD laser disc optical disc digital versatile disc DVD floppy disk and Blu ray disc where disks usually reproduce data magnetically while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer readable media.

For example the computer readable medium including program code stored thereon may include program code to display an image captured by a camera of a 2D surface with a 3D object program code to detect and track the 2D surface in the image program code to detect an occlusion of a region assigned as an area of interest on the 2D surface program code to determine a shape of the 3D object and program code to render and display a graphical object with reference to a location of the area of interest on the 2D surface wherein the graphical object is rendered with respect to the shape of the 3D object.

Although the present invention is illustrated in connection with specific embodiments for instructional purposes the present invention is not limited thereto. Various adaptations and modifications may be made without departing from the scope of the invention. Therefore the spirit and scope of the appended claims should not be limited to the foregoing description.

