---

title: Method and system for managing weakly mutable data in a distributed storage system
abstract: A method for managing multiple generations of an object within a distributed storage system is implemented at a computing device. The computing device receives metadata and content of a first generation of an object from a first client connected to the distributed storage system and stores the first generation's metadata and content within a first storage sub-system. The computing device receives metadata and content of a second generation of the object from a second client connected to the distributed storage system and stores the second generation's metadata and content within a second storage sub-system. The computing device independently replicates the first generation's metadata and content from the first storage sub-system to the second storage sub-system and replicates the second generation's metadata and content from the second storage sub-system to the first storage sub-system such that both storage sub-systems include a replica of the object's first and second generations.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08615485&OS=08615485&RS=08615485
owner: Google, Inc.
number: 08615485
owner_city: Mountain View
owner_country: US
publication_date: 20110209
---
This application claims priority to U.S. Provisional Application Ser. No. 61 302 918 filed Feb. 9 2010 entitled Method and System for Managing Weakly Mutable Data in a Distributed Storage System which is incorporated by reference herein in its entirety.

The disclosed embodiments relate generally to database replication and more specifically to method and system for managing weakly mutable data in a distributed storage system.

For weakly mutable data changes or mutations at one instance or replica of the data must ultimately replicate to all other instances of the database but there is no strict time limit on when the updates must occur. This is an appropriate model for certain data that does not change often particular when there are many instances of the database at locations distributed around the globe.

Replication of large quantities of data on a planetary scale can be both slow and inefficient. In particular the long haul network paths have limited bandwidth. In general a single change to a large piece of data entails transmitting that large piece of data through the limited bandwidth of the network. Furthermore the same large piece of data is transmitted to each of the database instances which multiplies the bandwidth usage by the number of database instances.

In addition network paths and data centers sometimes fail or become unavailable for periods of time both unexpected outages as well as planned outages for upgrades etc. . Generally replicated systems do not handle such outages gracefully often requiring manual intervention. When replication is based on a static network topology and certain links become unavailable or more limited replication strategies based on the original static network may be inefficient or ineffective.

The above deficiencies and other problems associated with replicating data for a distributed database to multiple replicas across a widespread distributed system are addressed by the disclosed embodiments. In some of the disclosed embodiments changes to an individual piece of data are tracked as deltas and the deltas are transmitted to other instances of the database rather than transmitting the piece of data itself. In some embodiments reading the data includes reading both an underlying value and any subsequent deltas and thus a client reading the data sees the updated value even if the deltas has not been incorporated into the underlying data value. In some embodiments distribution of the data to other instances takes advantage of the network tree structure to reduce the amount of data transmitted across the long haul links in the network.

In accordance with some embodiments a computer implemented method for managing multiple generations of an object within a distributed storage system is implemented at a computing device having one or more processors and memory. The memory stores one or more programs for execution by the one or more processors on the computing device which is associated with a distributed storage system that includes a plurality of storage sub systems.

The computing device receives metadata and content of a first generation of an object from a first client connected to the distributed storage system the first generation being identified by an object ID and a first generation ID and stores the first generation s metadata and content within a first storage sub system of the distributed storage system. The computing device receives metadata and content of a second generation of the object from a second client connected to the distributed storage system the second generation being identified by the same object ID and a second generation ID and stores the second generation s metadata and content within a second storage sub system of the distributed storage system. The computing device independently replicates the first generation s metadata and content from the first storage sub system to the second storage sub system and replicates the second generation s metadata and content from the second storage sub system to the first storage sub system such that both the first and second storage sub systems include a replica of the object s first and second generations. In some embodiments the first and second storage sub systems correspond to the same storage sub system.

In some embodiments the content of either the first generation or the second generation is immutable over time until the deletion of the respective generation. There may be at least one content difference between the first generation and the second generation.

In some embodiments each generation ID includes a first component that corresponds to a respective object generation s creation time and a second component that corresponds to a tie breaker that defines an order among multiple generations of the same object.

In some embodiments at either of the first and second storage sub systems the computing device compares the first generation s content with the second generation s content and deletes one of the first generation and the second generation from a respective storage sub system if the two generations have identical content. Note that both generations metadata can be replicated to another storage sub system e.g. a global storage sub system .

In some embodiments before the replication of the second generation s metadata from the second storage sub system to the first storage sub system the computing device receives a first client request for the latest generation of the object and returns the first generation s content and metadata to the requesting client in response to the first client request. After the replication of the second generation s metadata from the second storage sub system to the first storage sub system the computing device receives from a second client request for a latest generation of the object and returns the second generation s content and metadata to the requesting client in response to the second client request. The second generation s content is dynamically replicated from the second storage sub system to the first storage sub system in response to the second client request.

In some embodiments at either of the first and second storage sub systems the computing device determines a total number of generations of the object at the respective storage sub system and deletes one or more generations of the object from the respective storage sub system in accordance with their respective generation IDs if the total number of generations exceeds a predefined number associated with the object.

In accordance with some embodiments a computer system in association with a distributed storage system that includes a plurality of storage sub systems includes one or more processors memory and one or more programs stored in the memory for execution by the one or more processors. The one or more programs include instructions for receiving metadata and content of a first generation of an object from a first client connected to the distributed storage system wherein the first generation is identified by an object ID and a first generation ID storing the first generation s metadata and content within a first storage sub system of the distributed storage system receiving metadata and content of a second generation of the object from a second client connected to the distributed storage system wherein the second generation is identified by the object ID and a second generation ID storing the second generation s metadata and content within a second storage sub system of the distributed storage system and independently replicating the first generation s metadata and content from the first storage sub system to the second storage sub system and replicating the second generation s metadata and content from the second storage sub system to the first storage sub system such that both the first and second storage sub systems include a replica of the object s first and second generations.

In accordance with some embodiments a computer readable storage medium stores one or more programs configured for execution by a server computer system having one or more processors and memory storing one or more programs for execution by the one or more processors in association with a distributed storage system that includes a plurality of storage sub systems. The one or more programs comprise instructions to receive metadata and content of a first generation of an object from a first client connected to the distributed storage system wherein the first generation is identified by an object ID and a first generation ID store the first generation s metadata and content within a first storage sub system of the distributed storage system receive metadata and content of a second generation of the object from a second client connected to the distributed storage system wherein the second generation is identified by the object ID and a second generation ID store the second generation s metadata and content within a second storage sub system of the distributed storage system and independently replicate the first generation s metadata and content from the first storage sub system to the second storage sub system and replicate the second generation s metadata and content from the second storage sub system to the first storage sub system such that both the first and second storage sub systems include a replica of the object s first and second generations.

Thus methods and systems are provided that make replication of data in distributed databases faster and enable more efficient use of network resources. Faster replication results in providing users with updated information or access to information more quickly and more efficient usage of network bandwidth leaves more bandwidth available for other tasks making other processes run faster.

Reference will now be made in detail to embodiments examples of which are illustrated in the accompanying drawings. In the following detailed description numerous specific details are set forth in order to provide a thorough understanding of the present invention. However it will be apparent to one of ordinary skill in the art that the present invention may be practiced without these specific details.

The terminology used in the description of the invention herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the invention. As used in the description of the invention and the appended claims the singular forms a an and the are intended to include the plural forms as well unless the context clearly indicates otherwise. It will also be understood that the term and or as used herein refers to and encompasses any and all possible combinations of one or more of the associated listed items. It will be further understood that the terms comprises and or comprising when used in this specification specify the presence of stated features steps operations elements and or components but do not preclude the presence or addition of one or more other features steps operations elements components and or groups thereof.

The present specification describes a distributed storage system. In some embodiments as illustrated in the distributed storage system is implemented on a global or planet scale. In these embodiments there is a plurality of instances . . . N at various locations on the Earth connected by network communication links . . . M. In some embodiments an instance such as instance corresponds to a data center. In other embodiments multiple instances are physically located at the same data center. Although the conceptual diagram of shows a limited number of network communication links etc. typical embodiments would have many more network communication links. In some embodiments there are two or more network communication links between the same pair of instances as illustrated by links and between instance and instance . In some embodiments the network communication links are composed of fiber optic cable. In some embodiments some of the network communication links use wireless technology such as microwaves. In some embodiments each network communication link has a specified bandwidth and or a specified cost for the use of that bandwidth. In some embodiments statistics are maintained about the transfer of data across one or more of the network communication links including throughput rate times of availability reliability of the links etc. Each instance typically has data stores and associated databases as shown in and utilizes a farm of server computers instance servers see to perform all of the tasks. In some embodiments there are one or more instances that have limited functionality such as acting as a repeater for data transmissions between other instances. Limited functionality instances may or may not have any of the data stores depicted in .

Each instance has one or more clock servers that provide accurate time. In some embodiments the clock servers provide time as the number of microseconds past a well defined point in the past. In some embodiments the clock servers provide time readings that are guaranteed to be monotonically increasing. In some embodiments each instance server stores an instance identifier that uniquely identifies itself within the distributed storage system. The instance identifier may be saved in any convenient format such as a 32 bit integer a 64 bit integer or a fixed length character string. In some embodiments the instance identifier is incorporated directly or indirectly into other unique identifiers generated at the instance. In some embodiments an instance stores a row identifier seed which is used when new data items are inserted into the database. A row identifier is used to uniquely identify each data item . In some embodiments the row identifier seed is used to create a row identifier and simultaneously incremented so that the next row identifier will be greater. In other embodiments unique row identifiers are created from a timestamp provided by the clock servers without the use of a row identifier seed. In some embodiments a tie breaker value is used when generating row identifiers or unique identifiers for data changes described below with respect to . In some embodiments a tie breaker is stored permanently in non volatile memory such as a magnetic or optical disk .

The elements described in are incorporated in embodiments of the distributed storage system illustrated in . In some embodiments the functionality described in is included in a blobmaster and metadata store . In these embodiments the primary data storage i.e. blobs is in the data stores and and managed by bitpushers . The metadata for the blobs is in the metadata store and managed by the blobmaster . The metadata corresponds to the functionality identified in . Although the metadata for storage of blobs provides an exemplary embodiment of the present invention one of ordinary skill in the art would recognize that the present invention is not limited to this embodiment.

The distributed storage system shown in includes certain global applications and configuration information as well as a plurality of instances . . . N. In some embodiments the global configuration information includes a list of instances and information about each instance. In some embodiments the information for each instance includes the set of storage nodes data stores at the instance the state information which in some embodiments includes whether the metadata at the instance is global or local and network addresses to reach the blobmaster and bitpusher at the instance. In some embodiments the global configuration information resides at a single physical location and that information is retrieved as needed. In other embodiments copies of the global configuration information are stored at multiple locations. In some embodiments copies of the global configuration information are stored at some or all of the instances. In some embodiments the global configuration information can only be modified at a single location and changes are transferred to other locations by one way replication. In some embodiments there are certain global applications such as the location assignment daemon see that can only run at one location at any given time. In some embodiments the global applications run at a selected instance but in other embodiments one or more of the global applications runs on a set of servers distinct from the instances. In some embodiments the location where a global application is running is specified as part of the global configuration information and is subject to change over time.

In some embodiments each instance has a blobmaster which is a program that acts as an external interface to the metadata table . For example an external user application can request metadata corresponding to a specified blob using client . Note that a blob i.e. a binary large object is a collection of binary data e.g. images videos binary files executable code etc. stored as a single entity in a database. This specification uses the terms blob and object interchangeably and embodiments that refer to a blob may also be applied to objects and vice versa. In general the term object may refer to a blob or any other object such as a database object a file or the like or a portion or subset of the aforementioned objects. In some embodiments every instance has metadata in its metadata table corresponding to every blob stored anywhere in the distributed storage system . In other embodiments the instances come in two varieties those with global metadata for every blob in the distributed storage system and those with only local metadata only for blobs that are stored at the instance . In particular blobs typically reside at only a small subset of the instances. The metadata table includes information relevant to each of the blobs such as which instances have copies of a blob who has access to a blob and what type of data store is used at each instance to store a blob. The exemplary data structures in illustrate other metadata that is stored in metadata table in some embodiments.

When a client wants to read a blob of data the blobmaster provides one or more read tokens to the client which the client provides to a bitpusher in order to gain access to the relevant blob. When a client writes data the client writes to a bitpusher . The bitpusher returns write tokens indicating that data has been stored which the client then provides to the blobmaster in order to attach that data to a blob. A client communicates with a bitpusher over network which may be the same network used to communicate with the blobmaster . In some embodiments communication between the client and bitpushers is routed according to a load balancer . Because of load balancing or other factors communication with a blobmaster at one instance may be followed by communication with a bitpusher at a different instance. For example the first instance may be a global instance with metadata for all of the blobs but may not have a copy of the desired blob. The metadata for the blob identifies which instances have copies of the desired blob so in this example the subsequent communication with a bitpusher to read or write is at a different instance.

A bitpusher copies data to and from data stores. In some embodiments the read and write operations comprise entire blobs. In other embodiments each blob comprises one or more chunks and the read and write operations performed by a bitpusher are on solely on chunks. In some of these embodiments a bitpusher deals only with chunks and has no knowledge of blobs. In some embodiments a bitpusher has no knowledge of the contents of the data that is read or written and does not attempt to interpret the contents. Embodiments of a bitpusher support one or more types of data store. In some embodiments a bitpusher supports a plurality of data store types including inline data stores BigTable stores file server stores and tape stores . Some embodiments support additional other stores or are designed to accommodate other types of data stores as they become available or technologically feasible.

Inline stores actually use storage space in the metadata store . Inline stores provide faster access to the data but have limited capacity so inline stores are generally for relatively small blobs. In some embodiments inline stores are limited to blobs that are stored as a single chunk. In some embodiments small means blobs that are less than 32 kilobytes. In some embodiments small means blobs that are less than 1 megabyte. As storage technology facilitates greater storage capacity even blobs that are currently considered large may be relatively small compared to other blobs.

BigTable stores store data in BigTables located on one or more BigTable database servers . BigTables are described in several publicly available publications including Bigtable A Distributed Storage System for Structured Data Fay Chang et al OSDI 2006 which is incorporated herein by reference in its entirety. In some embodiments the BigTable stores save data on a large array of servers .

File stores store data on one or more file servers . In some embodiments the file servers use file systems provided by computer operating systems such as UNIX. In other embodiments the file servers implement a proprietary file system such as the Google File System GFS . GFS is described in multiple publicly available publications including The Google File System Sanjay Ghemawat et al. SOSP 03 Oct. 19 22 2003 which is incorporated herein by reference in its entirety. In other embodiments the file servers implement NFS Network File System or other publicly available file systems not implemented by a computer operating system. In some embodiments the file system is distributed across many individual servers to reduce risk of loss or unavailability of any individual computer.

Tape stores store data on physical tapes . Unlike a tape backup the tapes here are another form of storage. This is described in greater detail in co pending U.S. Provisional Patent Application Ser. No. 61 302 909 filed Feb. 9 2010 subsequently filed as U.S. patent application Ser. No. 13 023 498 filed Feb. 8 2011 Method and System for Providing Efficient Access to a Tape Storage System which is incorporated herein by reference in its entirety. In some embodiments a Tape Master application assists in reading and writing from tape. In some embodiments there are two types of tape those that are physically loaded in a tape device so that the tapes can be robotically loaded and those tapes that physically located in a vault or other offline location and require human action to mount the tapes on a tape device. In some instances the tapes in the latter category are referred to as deep storage or archived. In some embodiments a large read write buffer is used to manage reading and writing data to tape. In some embodiments this buffer is managed by the tape master application . In some embodiments there are separate read buffers and write buffers. In some embodiments a client cannot directly read or write to a copy of data that is stored on tape. In these embodiments a client must read a copy of the data from an alternative data source even if the data must be transmitted over a greater distance.

In some embodiments there are additional other stores that store data in other formats or using other devices or technology. In some embodiments bitpushers are designed to accommodate additional storage technologies as they become available.

Each of the data store types has specific characteristics that make them useful for certain purposes. For example inline stores provide fast access but use up more expensive limited space. As another example tape storage is very inexpensive and provides secure long term storage but a client cannot directly read or write to tape. In some embodiments data is automatically stored in specific data store types based on matching the characteristics of the data to the characteristics of the data stores. In some embodiments users who create files may specify the type of data store to use. In other embodiments the type of data store to use is determined by the user application that creates the blobs of data. In some embodiments a combination of the above selection criteria is used. In some embodiments each blob is assigned to a storage policy and the storage policy specifies storage properties. A blob policy may specify the number of copies of the blob to save in what types of data stores the blob should be saved locations where the copies should be saved etc. For example a policy may specify that there should be two copies on disk Big Table stores or File Stores one copy on tape and all three copies at distinct metro locations. In some embodiments blob policies are stored as part of the global configuration and applications .

In some embodiments each instance has a quorum clock server which comprises one or more servers with internal clocks. The order of events including metadata deltas is important so maintenance of a consistent time clock is important. A quorum clock server regularly polls a plurality of independent clocks and determines if they are reasonably consistent. If the clocks become inconsistent and it is unclear how to resolve the inconsistency human intervention may be required. The resolution of an inconsistency may depend on the number of clocks used for the quorum and the nature of the inconsistency. For example if there are five clocks and only one is inconsistent with the other four then the consensus of the four is almost certainly right. However if each of the five clocks has a time that differs significantly from the others there would be no clear resolution.

In some embodiments each instance has a replication module which identifies blobs or chunks that will be replicated to other instances. In some embodiments the replication module may use one or more queues . . . Items to be replicated are placed in a queue and the items are replicated when resources are available. In some embodiments items in a replication queue have assigned priorities and the highest priority items are replicated as bandwidth becomes available. There are multiple ways that items can be added to a replication queue . In some embodiments items are added to replication queues when blob or chunk data is created or modified. For example if an end user modifies a blob at instance then the modification needs to be transmitted to all other instances that have copies of the blob. In embodiments that have priorities in the replication queues replication items based on blob content changes have a relatively high priority. In some embodiments items are added to the replication queues based on a current user request for a blob that is located at a distant instance. For example if a user in California requests a blob that exists only at an instance in India an item may be inserted into a replication queue to copy the blob from the instance in India to a local instance in California. That is since the data has to be copied from the distant location anyway it may be useful to save the data at a local instance. These dynamic replication requests receive the highest priority because they are responding to current user requests. The dynamic replication process is described in more detail in co pending U.S. Provisional Patent Application Ser. No. 61 302 896 filed Feb. 9 2010 subsequently filed as U.S. patent application Ser. No. 13 022 579 filed Feb. 7 2011 Method and System for Dynamically Replicating Data Within a Distributed Storage System which is incorporated herein by reference in its entirety.

In some embodiments there is a background replication process that creates and deletes copies of blobs based on blob policies and blob access data provided by a statistics server . The blob policies specify how many copies of a blob are desired where the copies should reside and in what types of data stores the data should be saved. In some embodiments a policy may specify additional properties such as the number of generations of a blob to save or time frames for saving different numbers of copies. E.g. save three copies for the first 30 days after creation then two copies thereafter. Using blob policies together with statistical information provided by the statistics server a location assignment daemon determines where to create new copies of a blob and what copies may be deleted. When new copies are to be created records are inserted into a replication queue with the lowest priority. The use of blob policies and the operation of a location assignment daemon are described in more detail in co pending U.S. Provisional Patent Application Ser. No. 61 302 936 filed Feb. 9 2010 subsequently filed as U.S. patent application Ser. No. 13 022 290 filed Feb. 7 2011 System and Method for managing Replicas of Objects in a Distributed Storage System which is incorporated herein by reference in its entirety.

Each of the above identified elements may be stored in one or more of the previously mentioned memory devices and corresponds to a set of instructions for performing a function described above. The above identified modules or programs i.e. sets of instructions need not be implemented as separate software programs procedures or modules and thus various subsets of these modules may be combined or otherwise re arranged in various embodiments. In some embodiments memory may store a subset of the modules and data structures identified above. Furthermore memory may store additional modules or data structures not described above.

Although shows an instance server used for performing various operations or storing data as illustrated in is intended more as functional description of the various features which may be present in a set of one or more computers rather than as a structural schematic of the embodiments described herein. In practice and as recognized by those of ordinary skill in the art items shown separately could be combined and some items could be separated. For example some items shown separately in could be implemented on individual computer systems and single items could be implemented by one or more computer systems. The actual number of computers used to implement each of the operations databases or file storage systems and how features are allocated among them will vary from one implementation to another and may depend in part on the amount of data at each instance the amount of data traffic that an instance must handle during peak usage periods as well as the amount of data traffic that an instance must handle during average usage periods.

To provide faster responses to clients and to provide fault tolerance each program or process that runs at an instance is generally distributed among multiple computers. The number of instance servers assigned to each of the programs or processes can vary and depends on the workload. provides exemplary information about a typical number of instance servers that are assigned to each of the functions. In some embodiments each instance has about 10 instance servers performing as blobmasters. In some embodiments each instance has about 100 instance servers performing as bitpushers. In some embodiments each instance has about 50 instance servers performing as BigTable servers. In some embodiments each instance has about 1000 instance servers performing as file system servers. File system servers store data for file system stores as well as the underlying storage medium for BigTable stores . In some embodiments each instance has about 10 instance servers performing as tape servers. In some embodiments each instance has about 5 instance servers performing as tape masters. In some embodiments each instance has about 10 instance servers performing replication management which includes both dynamic and background replication. In some embodiments each instance has about 5 instance servers performing as quorum clock servers.

Although the storage shown in relates to metadata for blobs the same process is applicable to other non relational databases such as columnar databases in which the data changes in specific ways. For example an access control list may be implemented as a multi byte integer in which each bit position represents an item location or person. Changing one piece of access information does not modify the other bits so a delta to encode the change requires little space. In alternative embodiments where the data is less structured deltas may be encoded as instructions for how to make changes to a stream of binary data. Some embodiments are described in publication RFC 3284 The VCDIFF Generic Differencing and Compression Data Format The Internet Society 2002. One of ordinary skill in the art would thus recognize that the same technique applied here for metadata is equally applicable to certain other types of structured data.

A change to metadata at one instance is replicated to other instances. The actual change to the base value may be stored in various formats. In some embodiments data structures similar to those in are used to store the changes but the structures are modified so that most of the fields are optional. Only the actual changes are filled in so the space required to store or transmit the delta is small. In other embodiments the changes are stored as key value pairs where the key uniquely identifies the data element changed and the value is the new value for the data element.

In some embodiments where the data items are metadata for blobs deltas may include information about forwarding. Because blobs may be dynamically replicated between instances at any time and the metadata may be modified at any time as well there are times that a new copy of a blob does not initially have all of the associated metadata. In these cases the source of the new copy maintains a forwarding address and transmits deltas to the instance that has the new copy of the blob for a certain period of time e.g. for a certain range of sequence identifiers .

The overall metadata structure includes three major parts the data about blob generations the data about blob references and inline data . In some embodiments read tokens are also saved with the metadata but the read tokens are used as a means to access data instead of representing characteristics of the stored blobs.

The blob generations can comprise one or more generations of each blob. In some embodiments the stored blobs are immutable and thus are not directly editable. Instead a change of a blob is implemented as a deletion of the prior version and the creation of a new version. Each of these blob versions etc. is a generation and has its own entry. In some embodiments a fixed number of generations are stored before the oldest generations are physically removed from storage. In other embodiments the number of generations saved is set by a blob policy . A policy can set the number of saved generations as 1 meaning that the old one is removed when a new generation is created. In some embodiments removal of old generations is intentionally slow providing an opportunity to recover an old deleted generation for some period of time. The specific metadata associated with each generation is described below with respect to .

Blob references can comprise one or more individual references etc. Each reference is an independent link to the same underlying blob content and each reference has its own set of access information. In most cases there is only one reference to a given blob. Multiple references can occur only if the user specifically requests them. This process is analogous to the creation of a link a hard link in a desktop file system. The information associated with each reference is described below with respect to .

Inline data comprises one or more inline data items etc. Inline data is not metadata it is the actual content of the saved blob to which the metadata applies. For blobs that are relatively small access to the blobs can be optimized by storing the blob contents with the metadata. In this scenario when a client asks to read the metadata the blobmaster returns the actual blob contents rather than read tokens and information about where to find the blob contents. Because blobs are stored in the metadata table only when they are small there is generally at most one inline data item for each blob. The information stored for each inline data item is described below in .

As illustrated in the embodiment of each generation includes several pieces of information. In some embodiments a generation number or generation ID uniquely identifies the generation. The generation number can be used by clients to specify a certain generation to access. In some embodiments if a client does not specify a generation number the blobmaster will return information about the most current generation. In some embodiments each generation tracks several points in time. Specifically some embodiments track the time the generation was created . Some embodiments track the time the blob was last accessed by a user . In some embodiments last access refers to end user access and in other embodiments last access includes administrative access as well. Some embodiments track the time the blob was last changed . In some embodiments that track when the blob was last changed changes apply only to metadata because the blob contents are immutable. Some embodiments provide a block flag that blocks access to the generation. In these embodiments a blobmaster would still allow access to certain users or clients who have the privilege or seeing blocked blob generations. Some embodiments provide a preserve flag that will guarantee that the data in the generation is not removed. This may be used for example for data that is subject to a litigation hold or other order by a court. In addition to these individual pieces of data about a generation a generation has one or more representations . The individual representations etc. are described below with respect to .

In some embodiments each reference has its own blob policy which may be specified by a policy ID . The blob policy specifies the number of copies of the blob where the copies are located what types of data stores to use for the blobs etc. When there are multiple references the applicable policy is the union of the relevant policies. For example if one policy requests 2 copies at least one of which is in Europe and another requests 3 copies at least one of which is in North America then the minimal union policy is 3 copies with at least one in Europe and at least one in North America. In some embodiments individual references also have a block flag and preserve flag which function the same way as block and preserve flags and defined for each generation. In addition a user or owner of a blob reference may specify additional information about a blob which may include on disk information or in memory information . A user may save any information about a blob in these fields.

When a blob is initially created it goes through several phases and some embodiments track these phases in each representation data item . In some embodiments a finalization status field indicates when the blob is UPLOADING when the blob is FINALIZING and when the blob is FINALIZED. Most representation data items will have the FINALIZED status. In some embodiments certain finalization data is stored during the finalization process.

One important function of a distributed storage system as shown in is to store data objects uploaded from client applications at different geographical locations. To improve the upload efficiency a large data object may be divided into multiple chunks and different chunks of the data object may be uploaded into different chunk stores within the distributed storage system in parallel. Therefore both the client application and the distributed storage system need to have an efficient sanity check mechanism to ensure that the data object has been successfully uploaded into the distributed storage system before the object can be replicated at any instance within the distributed storage system.

In particular depicts a block diagram illustrative of how a blob is uploaded from a client into a blobstore with showing the corresponding flowcharts of this uploading process. depict block diagrams of data structures used by different components of the distributed storage system to support this process. For illustrative purposes depicts a subset of components of the distributed storage system as shown in including an application a client a blobstore and a LAD . Note that the term blobstore in this application corresponds to an instance of the system because it stores a plurality of blobs each blob being a data object e.g. a database record an image a text document or an audio video stream that is comprised of one or more chunks.

As shown in the application invokes the client to write a blob into the distributed storage system of . In some embodiments the client is created by the application for writing a data object provided by the application into the distributed storage system . The client has an application programming interface API for the application to write data associated with the object into a client cache of the client . The application repeatedly writes data into the client cache until it completes the last byte of the data object of . The client stores the received data in the client cache of . In some embodiments if the client cache is full the application may receive a message from the client to temporarily suspend sending more data to the client until it receives another message from the client indicating that more space is available for receiving data at the client cache .

Before uploading any data into the distributed storage system the client determines whether the blob should be partitioned into multiple chunks or not based on the blob s size of . In some embodiments different considerations may support different chunk sizes. For example the client may favor a relatively small chunk size if the distributed storage system uses a content based chunk de duplication module to eliminate identical chunks because a relatively small chunk size makes it more likely for two chunks to be identical. As such a chunk is shared by as many blobs as possible which could increase the storage efficiency of the distributed storage system. Conversely the smaller the chunk size the more chunks per blob which would increase the distributed storage system s overhead for managing a blob. Therefore a chunk size should be chosen to optimize the overall performance of the distributed storage system.

In some embodiments the chunk size is set to be multiple megabytes and all the chunks have the same chunk size regardless of the blobs being processed. This approach makes it easy for the client to divide an incoming blob. As long as the data accumulated in the client cache reaches a predefined chunk size a new chunk is ready to be uploaded into the blobstore . In some other embodiments more complex algorithms can be used to choose a chunk size at a blob level such that different blobs may have different chunk sizes. In some embodiments a chunk size can be determined at a chunk level such that different chunks within the same blob may have different sizes. One of the known algorithms for choosing the chunk size is the Rabin Karp algorithm that uses hashing to find any one of a set of pattern strings e.g. chunks in a text e.g. a blob . For each partitioned chunk the client specifies a chunk ID which in some embodiments is a multi bit e.g. 160 hash of the chunk s content a chunk offset which indicates the chunk s location within the blob and a chunk size.

In some embodiments the client generates a chunk metadata record for the blob s chunks in the chunk metadata table . The chunk metadata record includes the chunk related metadata some of which is to be uploaded into the blobstore s metadata table . depicts an exemplary chunk metadata record that includes multiple attributes a blob base ID a blob reference ID a replication policy the blob s access control information and one or more entries corresponding to the chunks of the blob. For each chunk the entry includes a chunk ID a chunk offset a chunk size and a write token . Note that the write tokens are provided by the blobstore for each chunk that has been successfully uploaded into the blobstore . Therefore a chunk that still resides in the client cache and has not been uploaded into the blobstore may not have a write token. The blob base ID identifies a particular generation of the blob being uploaded sometimes in combination with a blob generation ID if the blob has multiple generations . The blob reference ID indicates that the blob being uploaded is now being referenced by the application .

In some embodiments the client also determines a storage type for the blob based on e.g. the blob s replication policy and the blob s size of . As described above in connection with a particular instance of the distributed storage system includes multiple types of chunk stores such as inline store bigtable store file stores and tape stores . These different storage types are designed for different types of blobs and chunks. In some embodiments for a blob that requires quick access the client may specify an express policy that the blob should be stored in the inline chunk stores of a corresponding blobstore. In some other embodiments the client may adopt a more implicit policy that determines a chunk store type in accordance with the blob size. For example if the blob size is less than a first threshold level the chunks should be kept in an inline store whose overhead is low and access efficiency is high if the blob size is equal to or greater than the first threshold level but less than a second threshold level the chunks should be kept in a bigtable store which is generally more efficient than the file stores when handling blobs within certain size limit if the blob size is equal to or greater than the second threshold level the chunks should be kept in a file store which is generally more efficient for handling large blobs since the overhead is less sensitive to the blob size . Note that there can be multiple instances of chunk stores having the same store type in one blobstore. Therefore each chunk store has a unique chunk store ID and a combination of a blobstore ID and a chunk store ID uniquely identifies a particular chunk store within a particular blobstore.

In some embodiments the client identifies a load balanced blobmaster and a load balanced bitpusher of the blobstore of . At predefined moments such as when the client cache reaches a predefined limit of or when the application expressly instructs the client to upload the data in the client cache into the blobstore of the client then contacts the load balanced bitpusher and sends the chunk data to the bitpusher of . The bitpusher verifies whether the application is authorized to perform write operations to the blobstore of . If not the bitpusher returns an error message to the client and the blob write operation is therefore terminated. Otherwise the bitpusher writes the client provided chunks into the chunk stores of the blobstore . In some embodiments before writing a chunk into any chunk store the bitpusher also checks whether the chunk already has an identical replica in the blobstore as part of a content based chunk de duplication step. If an existing chunk of identical content is found in a respective chunk store the bitpusher may return the write token of the existing chunk to the requesting client .

Depending on the load balance at the blobstore the bitpusher may write different chunks of the same blob into one or more chunk stores and as shown in . If there are multiple chunk stores of the same storage type e.g. bigtable store that can host new chunks it would be more efficient for the bitpusher or may be multiple load balanced bitpushers to write different chunks into different chunk stores in parallel. For each chunk the bitpusher returns a write token to the client of . In some embodiments the write token is a cryptographically signed token that includes information about a respective chunk store that hosts a respective chunk of a respective blob.

Upon receipt of the write tokens the client generates an extents table for the blob using the write tokens of . In some embodiments the extents table is based on the chunk metadata record . For each uploaded chunk there is a corresponding entry in the extents table that includes the chunk related attributes such as chunk ID chunk offset and chunk size. The offsets of different chunks define a sequence for the chunks such that a concatenation of the chunks in accordance with this sequence covers the entire blob. In some embodiments when the client receives two write tokens that correspond to two consecutive but non contiguous chunks defined by the sequence the client inserts a dummy entry into the extents table to indicate that there is a gap of missing data between the two chunks. In some embodiments the dummy entry corresponds to a dummy chunk that has a predefined chunk ID a chunk offset that corresponds to the gap s location within the blob and the a chunk size that indicates the amount of missing data.

In some embodiments the dummy entry corresponds to a dummy chunk i.e. a non existing chunk with a chunk ID a chunk offset and chunk size. For example assuming that a blob has two chunks A and B and the two chunks have been uploaded into the blobstore the extents table of the blob may initially be expressed as follows 

Note that the two chunks are not contiguous and there is a gap of 50 bytes from the end of Chunk A to the beginning of Chunk B the client may insert another entry into the extents table corresponding to a dummy chunk as follows 

In some embodiments the dummy entries in the extents table are used as a placeholder by the client to suggest that the blob has some missing data. Of course there will be no dummy entries in the extents table if the blob being uploaded has no missing data and any two consecutive chunks are always contiguous.

In some embodiments the client also uploads the metadata in the chunk metadata table into the blobstore by sending the metadata to the blobmaster of . In some embodiments this occurs after the client has uploaded all the chunks into the blobstore of and therefore there is a chunk metadata record and a corresponding entry in the extents table for every uploaded chunk. In some other embodiments the metadata upload is triggered by an express request from the application of . In this case the client may perform multiple metadata upload operations at different times. Using the uploaded metadata the blobmaster generates a new entry in the metadata table of . as described above depict the data structures of a blob s metadata entry in the metadata table . In particular one upload of a blob corresponds to one generation of the blob. Depending on its replication policy a blob may have multiple generations and each generation has its own set of chunks in a respective chunk store. The finalization status of a blob indicates the current status of the blob in the blobstore. Initially the blobmaster marks the finalization status of the newly generated entry in the metadata table as being uploading indicating that not all the chunks of the same blob have been uploaded to the blobstore .

In some embodiments the client provides the blob metadata including all the write tokens received by the client the blob s access control list extents table and replication policy as well as the actual chunk contents if the client determines that the blob s chunks should be stored as inline data of . As soon as the blobmaster inserts the metadata into the metadata table of the blobmaster is ready for receiving and processing other access requests to the blob.

In some embodiments after uploading the last chunk into the blobstore e.g. upon receipt of the corresponding write token the client sends a notification to the blobmaster to finalize the blob at a client specified destination chunk store of . In addition to making this request the client also clears its client cache for other use. In some embodiments the client determines the destination chunk store by examining the write tokens and the extents table to determine whether all the chunks are located within the same chunk store e.g. chunk store A or chunk store Z . If they are not located within the same chunk store the client may identify a chunk store that has more chunks of the same blob than any other chunk store as the destination chunk store and provide information about the destination chunk store as part of its request to finalize the blob uploading. depicts an exemplary data structure of a chunk store metadata record that includes a blob base ID a blob reference ID and the number of bytes for each chunk store that has at least one chunk for a corresponding blob. From this record the client can easily tell which chunk store has the highest number of chunks for a respective blob and then choose the chunk store as the destination chunk store.

In some embodiments the distributed storage system requires that all chunks related to the same blob be within the same chunk store to make the other operations e.g. chunk deletion or chunk replication more efficient. Accordingly the blobstore needs to relocate chunks at different chunk stores into the same chunk store which is also referred to as the destination chunk store or canonical chunk store . Based on the client provided information the blobmaster determines whether all the chunks are within the same chunk store or not of . If true the blobmaster then marks the finalization status of the blob in the metadata table as finalize of . In addition the blobmaster notifies the LAD that the blob is ready to be replicated to the other blobstores in accordance with the blob s replication policy of . If false the blobmaster marks the finalization status as finalizing of and triggers the repqueue to perform the chunk level replication by moving the chunks from those non destination chunk stores to the destination chunk store of . For each relocated chunk the repqueue sends a metadata update to the blobmaster for updating the blob s extents table in the metadata table of . This chunk relocation process is supported by the additional data structures as shown in .

In particular depicts an exemplary chunk replication request that includes multiple attributes a replication ID a blob base ID a blob generation ID a representation type a chunks list and a replication priority . In some embodiments the replication ID further includes a source chunk store ID that identifies a chunk store within the blobstore a destination chunk store ID that identifies a chunk store within the same blobstore a user ID that initiates the replication and a network quality of service parameter. A combination of the blob base ID and the blob generation ID uniquely identifies a particular generation of the blob to be replicated. In some embodiments both parameters are originally provided by the client who initiates the access request for the blob. The chunks list typically includes one or more pairs of chunk ID and chunk sequence ID each pair uniquely identifying a chunk within the corresponding chunk store. The replication priority indicates whether this is a high priority real time dynamic replication or a low priority background replication.

For each chunk relocated from the chunk store A to the chunk store Z the bitpusher generates a chunk index record for the new chunk using the chunk reference record at the chunk index table and inserts the chunk index record into the chunk store Z s chunk index table .

For each newly relocated chunk the blobmaster checks the extents table to determine if the chunks associated with the same blob are all within the same chunk store Z of the blobstore . If so the blobmaster updates the finalization status of the blob from finalizing to finalized indicating that the uploading of the client provided object from the client to the blobstore is successfully completed.

In some embodiments the blobmaster determines whether the blob can be finalized based on a digest of extents table provided by the client . As described above the client inserts a dummy entry into the extents table for each pair of consecutive but non contiguous chunks. When the client decides that all the chunks have been uploaded it sends to the blobmaster a digest of the blob s extents table as part of the blob finalization request. depicts an exemplary data structure of a blob digest record that includes a blob base ID a blob reference ID a total number of chunks of the blob not including the dummy chunks and a hash of all the entries in the extents table described above. Upon receipt of the digest the blobmaster compares the total number of chunks which is the number of chunks that the client expects to be within the distributed storage system and the number of chunks it has e.g. in accordance with the number of read tokens in the blob s metadata record . If the two numbers do not match the blobmaster returns an error message to the client indicating that it does not have all the chunks at place. In response the client may resend the chunks missing from the blobstore. As such the blobmaster can make a quick determination as to the status of the uploaded chunks without calculating another digest of the extents table using the blob s metadata in the metadata table and then comparing the two digests which is a more expensive operation. Note that this content consistency protection mechanism works regardless of how a chunk ID is defined as long as both the client and the blobmaster share the same definition on the chunk ID. The hash of the content based chunk IDs for an entire blob effectively encodes the blob s content rather than the contents of one or more chunks which provides an improved security on the comparison result.

In some embodiments the client does not insert a dummy entry into its extents table for each gap between two consecutive but non contiguous chunks as described above. Each entry in the extents table corresponds to an actual chunk that has been uploaded into the distributed storage system. In this case a digest of the extents table implicitly states whether there is any data missing from an uploaded blob. By comparing the client provided digest and a digest generated by the blobmaster it is still possible for identifying what range of data is missing from the blob originally processed by the client . The blobmaster maintains the information for any future client access to the blob to protect its data integrity.

In some embodiments a blob s content does not change over time until the deletion of the blob from the distributed storage system . One way of allowing the mutation of an existing blob s content in the distributed storage system is to overwrite the blob with a new blob and assign the same blob ID to the new blob. As described above in connection with the multiple generation data items in a blob metadata entry are used to support the mutation of blob content by allowing one blob to have or more generations different generations having the same blob base ID but different generations IDs. Thus the process of uploading a blob from a client into the distributed storage system as described above in connection with actually generates a new generation for the blob. Note that such instance of blob may not be the first generation of the blob because multiple clients can upload different instances of the same blob into different nearby blobstores in parallel. In this case it could be a challenge for the distributed storage system to maintain the data consistency between different blobstores.

As shown in a client A writes an object into a blobstore A of the distributed storage system . A detailed description of this blob uploading process is provided above in connection with . At the end of the process an instance of the object is generated within the blobstore A . For convenience this instance is referred to as the generation A that includes metadata A and content A . Note that the metadata A corresponds to a generation data item in the blob s metadata and the content A may be comprised of one or more chunks located at a particular chunk store of the blobstore A . In particular the generation data item includes a unique generation ID or number for the generation A. In some embodiments the generation ID has at least two components to uniquely identify a particular generation. The first of the two components corresponds to the blob instance s creation timestamp. Therefore blob instances generated at different times will have different generation IDs. But as will be explained below the asynchronous nature of the distributed storage system cannot prevent two generations from not having the same creation timestamp. As a result an instance s creation timestamp by itself may not be sufficient for uniquely identifying a particular generation. As such the second of the two components is essentially a tie breaker such that two generations of the same blob that have the same creation timestamp can be distinguished from each other by their respective tie breakers.

As noted above the LAD is responsible for scheduling the replication of the generation A of a blob across the distributed storage system in accordance with the blob s replication policy. For example if the blob s replication policy is that there should be two replicas of the same generation at two different blobstores another replica of the same generation A should appear at another blobstore e.g. blobstore B sometime in the future. The latency between the creation of the first replica at the blobstore A and the creation of the second replica at the blobstore B varies from a few minutes to a few hours depending on how busy the distributed storage system is.

Assuming that before the replication of the generation A a client uploads a new generation B of the same blob into the blobstore B . In some embodiments the two blob uploading processes occur in parallel. For example both the generation A and the generation B are uploaded into the respective blobstore to overwrite an existing instance of the same blob. In other words neither of the two generations is the first generation of a new blob. Therefore as shown in it is possible that at one point in time or even during a short time period the blobstore A includes a first set of generations of a blob including the generation A comprising the metadata A and the content A while the blobstore B includes a second set of generations of the same blob including the generation B comprising the metadata B and the content B . In other words the two blobstores have two different sets of generations.

In this case if one client sends a request for the most recent or latest generation of the blob to the blobstore A the blobstore A may give the generation A which is assumed to be the latest generation at the blobstore A to the client. Similarly a client that sends a request for the most recent or latest generation of the blob to the blobstore B will probably receive the generation B which is assumed to be the latest generation at the blobstore B .

From the perspective of system consistency the scenario above in which different clients receive different generations should be minimized as much as the system permits. As shown in the distributed storage system achieves this goal by first replicating the metadata of a new generation created at a particular blobstore. As a result the blobstore A receives a replica of the metadata B and the blobstore B receives a replica of the metadata A . In general the time it requires to replicate a blob s metadata is significantly shorter than the time it takes to replicate the blob s content. If a client s access request arrives at the blobstore A after it receives the metadata B and the blobstore A determines that the generation B identified by the metadata B is more recent than the generation A it will not return the generation A to the requesting client. Instead the blobstore A can refer the blobstore B to the requesting client which responds to the request with the generation B. Alternatively the blobstore A may initiate a dynamic replication process to directly request the content B from the blobstore B . In either case both the generation A and the generation B will be ultimately replicated from the respective source blobstore to the respective destination blobstore .

In some embodiments when a blobstore receives a new generation of an existing blob it compares the new generation s content with the content of any existing generation e.g. through content hash . If the new generation is deemed to have the same content as an existing generation the blobstore will merge the new generation with the existing one as part of content de duplication. In some embodiments a client can request the latest generation of a blob without specifying any generation ID. In some other embodiments the client can also request a specific generation of a blob by providing a generation ID in addition to the blob ID. In yet some other embodiments the client can even request multiple generations of the same blob if it specifies multiple generation IDs. As noted above the LAD schedules the replication of a blob in accordance with the blob s replication policy. In some embodiments a user can specify as part of a blob s replication policy a garbage collection policy for the blob e.g. the blob can at most keep three generations at one time . In this case the replication of a new generation across the distributed storage system also triggers the deletion of older generations if the blob s garbage collection policy requires so. As such the distributed storage system always keeps a limited set of the most recent generations for the blob.

The foregoing description for purpose of explanation has been described with reference to specific embodiments. However the illustrative discussions above are not intended to be exhaustive or to limit the invention to the precise forms disclosed. Many modifications and variations are possible in view of the above teachings. The embodiments were chosen and described in order to best explain the principles of the invention and its practical applications to thereby enable others skilled in the art to best utilize the invention and various embodiments with various modifications as are suited to the particular use contemplated.

