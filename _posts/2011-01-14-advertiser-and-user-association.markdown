---

title: Advertiser and user association
abstract: The subject matter of this specification can be embodied in, among other things, a method that includes generating content-based keywords based on content generated by users of a social network. The method includes labeling nodes comprising user nodes, which are representations of the users, with advertising labels comprising content-based keywords that coincide with advertiser-selected keywords that are based on one or more terms specified by an advertiser. The method also includes outputting, for each node, weights for the advertising labels based on weights of advertising labels associated with neighboring nodes, which are related to the node by a relationship.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08572099&OS=08572099&RS=08572099
owner: Google Inc.
number: 08572099
owner_city: Mountain View
owner_country: US
publication_date: 20110114
---
This application is a continuation of and claims priority to U.S. patent application Ser. No. 11 743 006 filed May 1 2007 the entire contents of which are hereby incorporated by reference.

Social networks can permit users to post information about themselves and communicate with other people such as their friends family and co workers. Some social networks permit users to specify friendships with other users. Additionally social network users may write descriptions of their interests compose stories describe their lives etc. Some user may generate a great amount of content while other users may generate very little.

In one general aspect a computer implemented method is described. The method includes generating content based keywords based on content generated by users of a social network. The method includes labeling nodes comprising user nodes which are representations of the users with advertising labels comprising content based keywords that coincide with advertiser selected keywords that are based on one or more terms specified by an advertiser. The method also includes outputting for each node weights for the advertising labels based on weights of advertising labels associated with neighboring nodes which are related to the node by a relationship.

In a second general aspect a computer implemented method is described where the method includes receiving selected keywords based on terms specified by advertisers for use in selecting online advertisements for display. The method also includes labeling a portion of user nodes representing users of a social network with the selected keywords and outputting for each user node a weight for the selected keywords based on weights of selected keywords associated with neighboring user nodes that are related to the user node by a social relationship.

In a third general aspect a system is described. The system includes a classifying module to label nodes comprising representations of the users of a social network with advertising labels comprising content keywords derived from content generated by the user. The advertising labels coincide with advertiser selected keywords that are based on one or more terms specified by an advertiser. The system also includes means for outputting for each node weights for the advertising labels based on weights of advertising labels associated with neighboring nodes which are related to the node by a relationship.

The systems and techniques described here may provide one or more of the following advantages. First large segments of social network users may be identified and accurately targeted for advertising. Second users and user interest s can be mapped in an automated fashion to advertisers and advertiser keywords. Third content can be associated with users and used to identify users interests even if the users did not generate the content. Fourth keywords that are not selected by advertisers can be mapped to advertisers or advertiser selected keywords to facilitate the targeting of users of a social network e.g. for advertising purposes .

The details of one or more embodiments are set forth in the accompanying drawings and the description below. Other features and advantages will be apparent from the description and drawings and from the claims.

This document describes systems and techniques for inferring users interests. Social networks can host information about and generated by the social network s users. For example as seen in users can create profiles which can include interests stories facts or descriptions about a user. Additionally the users can specify social relationships with other users. For example a user can specify other users that are friends. 

Advertising systems such as server system of can target online advertisements ads to person who view the user profiles hosted on the social network. In certain implementations the advertising system generates the online ads based on the content of the profiles. In some situations a user may not provide enough or any information in his profile which may make it difficult to generate online ads that are accurately targeted for the profile.

In the situation where a first user lacks information in his profile profiles of other users that are related to the first user can be used to generate online ads for display with the first user s profile. For example a first user Isaac may not have any information in his profile except that he has two friends Jacob and Esau. The server system can use information from Jacob and Esau s profiles to infer information for Isaac s profile. The inferred information can be used to generate online ads that are displayed when Isaac s profile is viewed.

In other situations profiles for social network users may include terms or colloquialisms that do not correspond to keywords that advertisers use to target their campaigns. These terms or colloquialisms can be mapped to keywords that are used by advertisers by inferring associations between the terms or colloquialisms collectively referred to as non advertising keywords and the keywords.

In some implementations the inferred associations between the non advertising keywords and the advertising keywords are based on social relationships specified in user profiles. For example Jacob may include terms in his profile such as hax0r i.e. hacker and btcn i.e. better than Chuck Norris that do not correspond to advertising keywords. Jacob s profile specifies that Leah and Rachel are friends. Both Leah and Rachel include terms in their profile such as Ajax i.e. asynchronous JavaScript and XML and Perl Practical Extraction and Reporting Language that do correspond to advertising keywords. An advertising system such as the server system can be used to generate associations between Leah and Rachel s terms and Jacob s terms so that advertisements targeted for Leah and Rachel based on the keywords in their profile can also be used to target ads for Jacob because an association is inferred between Jacob s profile and terms in Leah and Rachel s profiles.

In some implementations the social network system includes the user profiles which can include user generated content such as user interests user blogs postings by the user on her or other users profiles e.g. comments in a commentary section of a web page a user s selection of hosted audio images and other files and demographic information about the user such as age gender address etc.

Additionally the profiles can include social relationships that specify associations between users of the social network system . For example a user Joshua may list Aaron and Caleb as friends and may be a member of the Trumpet Player Society which includes Izzy as a second member. The specified friendship and group membership relationships can be used to infer a similarity in user interests between the users Joshua Aaron Caleb and Izzy.

Information or content of the profiles can be transmitted to the server system as indicated by an arrow . The server system can use an inferred label generator to create labels for incomplete or sparsely populated user profiles based on content of related user profiles e.g. profiles of friends or members of the same group clubs society etc. .

In some implementations the labels can include keywords associated with the profiles. For example a label may be a keyword such as cars that is included in a web page profile. In other implementations the labels may be categories associated with content in a profile for example a profile can include content describing Corvettes Vipers and Hugos. A label applied to the profile may be cars based on predetermined associations between the Corvette Viper and Hugo terms and the category cars. 

In the implementation of the server system includes a data store where information about the labels can be stored. In some implementations each user profile may be associated with more than one label. The associations between each profile and corresponding labels can be stored in a data structure such as a label value table .

In some implementations the label value table can also include label values that indicate a possible interest level for each label. For example Adam s profile can include content about gardening and animal husbandry but twice as much content may be focused on gardening. The label value table can include a label value of 0.66 for gardening and 0.33 for animal husbandry under Adam s profile entry which indicates the profile includes twice as much information on gardening as animal husbandry.

The data store can also include advertising information used to generate online advertisements ads directed to profiles hosted by the social network system . The online advertisements can be transmitted by an ad server hosted by the server system . The ad server can use the label information in the label value table to generate online ads that are targeted for user profiles based on labels associated with the user profiles. The server system can transmit as indicated by arrow the target ads to the social network system for display with the user profiles or to the user s associated with the user profiles.

In some implementations the server system can receive content of user profiles from a social network system as indicated by arrow . The server system can use the content to identify or generate a labels based on the content where the labels can be used to generate or identify online ads that are targeted to corresponding users or user profiles.

In some implementations the inferred label generator includes a data structure generator to create a data structure used to infer the labels. For example the data structure generator can generate a graph where each user or user profile is represented with a node. The data structure generator can include a relationship determination module that links the nodes with edges based on social relationships specified by the user. For example a user Adam may specify in his profile that Seth is a friend. The relationship determination module can join the user nodes for Adam and Seth with an edge. The edges may be bi directional or uni directional however for the purposes of clarity the edges in the following examples are bi directional unless otherwise specified.

The inferred label generator also includes a classification module which can associate users of the social network system with labels such as the predetermined labels in the second data store . For example the classification module can use text analysis techniques such as those described in A Comparative Study on Feature Selection in Text Categorization Source Yang Yiming and Pedersen Jan O. Proceedings of the Fourteenth International Conference on Machine Learning pages 412 420 Machine learning in automated text categorization Sebastiani Fabrizio ACM Computing Surveys CSUR archive Volume 34 Issue 1 March 2002 pages 1 47 and Text Categorization with Suport Vector Machines Learning with Many Relevant Features Joachims Thorsten Lecture Notes In Computer Science Vol. 1398 archive Proceedings of the 10th European Conference on Machine Learning pages 137 142 the entirety of each is incorporated by reference here. Text analysis can be used to determine labels to association with user nodes in a generated graph. The subject of the analysis can include user profiles comments posted by a user descriptions of groups to which a user belongs etc. In other implementations the predetermined labels can be based on keywords which are submitted by advertisers. For example the keywords can include the term furniture. This can be used as a predetermined label which the classifying module associates with users that have profiles that include the term furniture. Additionally the classifying module can associate the users with the label furniture if the user profiles include words that are associated with furniture such as chair table or home decorating.

In some implementations the inferred label generator also includes a label value modifier module . The label value modifier module can modify label values associated with users and label values associate with groups of users advertiser selected keywords groups of keywords advertisers and keywords not selected by advertisers using methods and structures more fully described below.

Initial and modified label values can in some implementations be stored in a data structure such as a label value table of the first data store . As shown in the label value table can include an index that associates label values with each of the following users groups of users keywords not selected by advertisers and groups of keywords selected by an advertiser.

The advertising server can use advertising information stored in the first data store to generate or select an electronic advertisement that is based on the label or label values associated with each user. The advertising information can include for example keywords that an advertiser selected to associate with their advertisement. The advertising information can also specify advertising word groups also referred to as Adgroups which are categories that include related keywords and are associated with ads to display when one of the keywords occurs for example in a user s profile. Additionally the advertising information can include information about the advertisers such as an identifier payment information bidding price for ads etc.

The method can begin with step where labels are selected to associate with users. For example labels can include both categories of interest e.g. music movies business etc. and keywords Britney Spears tables masks etc. . In some implementations either categories or keywords are selected as labels. Also a subset of categories or keywords can be selected from the superset to use as labels. For example a user may input a selection of particular keywords to the inferred label generator where the selection specifies that the inferred label generator should use the categories Racing Stocks music politics and religion as keywords.

In step labels selected in step are assigned to users based on content of electronic documents associated with the users. For example the classifying module can classify each user based on their user profile or other content that the user has created. The classifying module can use text classification machine learning algorithms to accomplish the classifications. In certain implementations a user can be classified using more than one label. Additionally in some implementations if a user s profile does not have enough information to classify the user is left unclassified.

Although not shown in method in some implementations if the social network system includes any user groups such as clubs societies classes etc. the classifying module can attempt to categorize the groups based on text and other content that describes the group or is associated with the group such as message boards for the group. The classifying module can classify the groups in a substantially similar way as the user classification is accomplished.

In step a data structure such as a graph is generated. For example the data structure generator can represent each user user group unselected keywords or categories advertising groups and advertisers as a node in the graph. The relationship determination module can link the nodes based on social and other relationships specified by the nodes. For example the relationship determination module can link two user nodes together if the users specified each other as friends on their user profiles. In another example an advertiser node can be linked to an advertising group that includes keywords that were specified by the advertiser.

In some implementations label nodes having multiple label values for different labels are also associated with user nodes. These label nodes inject label values into the user node e.g. see . This is described in more detail in association with the .

Additionally in some implementations the inferred label generator generates a second data structure such as a second graph. The second graph can also include nodes substantially similar to the first graph except that each user can be associated with one or more label nodes that express a single subject of interest e.g. see .

In step a user is selected. For example the inferred label generator can select a user node representing a user in a graph. The inferred label generator can access neighboring user nodes that are linked to the selected user node e.g. the user s friends and use their label values to generate a new or modified label value for the selected user as indicated in step .

If the selected user has additional labels that have not been assigned new or modified label values the inferred label generator selects the additional label and repeats step until all the labels are processed as indicated by step .

In step the label values for the user are normalized. For example the inferred label generator can normalize the label values to a figure between a 0 and 1 where the figure expresses the label value s contribution relative to the other label values contributions. For example if the pre normalized label values are Stocks 2.0 Racing 1.0 and Gambling 1.0 then the normalized values would be Stocks 0.5 Racing 0.25 and Gambling 0.25.

If there are additional users that have not been selected using the method the next unselected user can be selected as illustrated by step . For example the inferred label generator can select the next unselected user node. If all the users have been selected the method can end.

In some implementations steps can also be performed on the second graph described in association with step . The inferred label generator can select and compare the resulting label value magnitudes for a corresponding user node from both the first and second graphs. In some implementations the inferred label generator can combine the label value magnitudes from each graph through linear weighting to determine a final label value magnitude e.g. the first graph s label value contributes 0.7 and the second graph s label value contributes 0.3 . In other implementations the values can be weighed equally to determine the final label value e.g. 0.5 0.5 or the inferred label generator can give one value its full contribution while ignoring the value from the other graph e.g. 1.0 0.0 or 0.0 1.0 .

In other implementations the inferred label generator can use a cross validation method to set the contribution weights for label value magnitudes from each graph. For example the inferred label generator can access user nodes in a graph where the user nodes have label value magnitudes that are know. The inferred label generator can compare the actual label value magnitudes with the known label value magnitudes. The inferred label generator can then weight each graph based on how closely its values match the known label values.

In certain implementations the label generator can compare the label value magnitudes to an expected a priori distribution instead of or in addition to examining the final label magnitudes. For example if a summed weight of label A across all label nodes is 8.0 and the weight of label B across all of the label nodes is 4.0 the a priori distribution suggests that label A may be twice as likely to occur as label B. The inferred label generator can use this expectation to calibrate the label value magnitudes for each user node. If in node n Label A weight is 1.5 and label B weight is 1.0 then the evidence for label A although higher than label B is not as high as expected because the ratio is not as high as the a prior distribution. This decreases the confidence that the difference in magnitudes is meaningful. A confidence factor can be translated back into the rankings.

In some implementations if the difference in magnitudes is below a confidence threshold the inferred label generator can rank a label with a lower weight above a label with a higher weight e.g. manipulate the lower weight so that it is increased to a value greater than the higher weight . For example if label A s weight is expected to be three times the weight of label B but was only 1.1 times greater than label B the inferred label generator can rank label B above label A. In some implementations the confidence factor can be kept as a confidence measure which can be used for example by machine learning algorithms to weight the resultant label value magnitudes.

In yet other implementations instead of or in addition to comparing the label value magnitudes based on the a priori distribution of the label nodes the inferred label generator can compare the label value magnitudes based on an end distribution of magnitudes across all nodes. For example the inferred label generator can measure how different a particular node s distribution is from an average calculated across all nodes in a graph.

Given the labels generated by the inferred label generator in some implementations the ad server can use the labels to select ads to display. For example if a user node is associated with the label Britney Spears the ad server can select music ads to display with a user profile associated with the user node. In another example if the label is religion the ad server can select ads that are determined to be religious or ads that an advertiser specified to be displayed based on an occurrence of terms relating to religion.

In yet other implementations other factors can influence whether an ad is displayed based on the labels. For example the ad server can factor in how much an advertiser is willing to pay for display of the ad such as the cost per impression click transaction etc. to determine whether to display an advertisement and which advertisement to display.

This information can also be represented by an exemplary graph data structure as shown in . Each of the users in a social network can be represented as nodes in the graph. For example Abby Bob Cal Dale and Ernie each are represented by nodes and respectively. For simplicity of explanation the graph data structure does not explicitly show nodes associated with groups such as the VeganVets4Life and CannotDrive55 included in the table of . However labels associated with the groups e.g. Recycled Tofu Dogs and NASCAR are associated with the members of the groups. This is described in more detail below.

In certain implementations the user nodes are linked by edges based on social relationships specified by the users. For example Abby node is linked by edge to Cal node because Abby specified that Cal was a friend on her profile page. also shows label nodes that associate user nodes with labels that express a probable interest of the user. In some implementations the label node has label values that may not change based on the label values of neighboring nodes. This is illustrated in this figure by the uni directional edge .

For example Abby node is associated with label node having label values 0.66 for a label Dogs and 0.33 for a label Recycled Tofu shorted to Tofu in . In certain implementations the labels have label values that express how much each label should contribute to the user node. The label Dogs specifies that Abby is interested in Dogs based on the content of Abby s profile page. The label value for Dogs is increased because Abby is a member of the group VeganVets4Life which is also associated with the label Dogs. Because Abby is a member of VeganVets4Life Abby node is also associated with a label value for Recycled Tofu.

In this example the label value for Dogs is twice the value for Recycled Tofu because Abby is associated with the Dogs label through two means e.g. group membership in VeganVets4Life as well as the content of Abby s profile whereas she is only associated with Recycled Tofu through one source e.g. her group membership in VeganVets4Life .

Bob node is associated with a label node which specifies that Bob is equally interested in Racing and Stocks based on for example Bob s profile.

The label node is associated with Ernie node and specifies that Ernie is interested in Stocks Gambling and NASCAR. More specifically the label values specify that Ernie is interested in all three equally. In this example the Stocks and Gambling labels are derived from Ernie s profile and the NASCAR label is derived from Ernie s membership in the group CannotDrive55 as indicated by the table of . In this example the label values are normalized to a value between 0 and 1 where the contribution of the label is proportional to all the labels. Because Ernie is equally interested in Gambling Stocks and NASCAR each is assigned a label value of 0.33.

Additionally illustrated in are group nodes that are linked by edges to users based on user membership in the groups. The group nodes are linked to label nodes where for example the classifying module links the groups to the label nodes based on textual descriptions of the groups.

The method can be executed using information from the graphs shown in and can be executed for every node in the graphs. The method can start with step which determines if a specified number of iterations have run for a graph. If the number is not complete step is performed. In step a node is selected. For example the inferred label generator can select a node that has label values that may be modified by the algorithm such as user nodes variable label nodes group nodes etc.

In step a label is selected from the selected node. For example the inferred label generator can select the label Stocks if present in the selected node. In step a label value for the selected label is initialized to zero. For example the inferred label generator can set the label value for Stocks to zero.

In step a neighboring node of the selected node can be selected. For example the selected node may specify that Abby is a friend. The inferred label generator can select a neighboring user node representing Abby.

In step a corresponding weighted label value of a selected neighbor is added to a label value of the selected label. For example if Abby has a label value for the label Stocks the inferred label generator can add this value to the selected node s label value for Stocks. In certain implementations the label value retrieved from a neighboring node can be weighted to affect the contribution of the label value based on the degree of distance from the selected node e.g. based on whether the neighboring node is linked directly to the selected node linked by two edges to the selected node etc. In some implementations the label value can also be based on a weight associated with the edge. For example the edge weight can reflect an explicit association between two nodes. For instance a first node may specify that the associated user likes a friend Jack where the value associated with liking Jack is 0.8 that the user likes friend Jill half as much as Jack etc.

In other implementations the contribution can be weighted based on whether a link to a neighboring node is bidirectional e.g. the neighboring node also specifies the selected node as a friend .

In step it is determined whether there is another neighbor node to select. For example the inferred label generator can determine if the selected node is linked by a single edge to any additional neighbors that have not been selected. In another example a user may specify how many degrees out e.g. linked by two edges three edges etc. the inferred label generator should search for neighbors. If there is another neighbor that has not been selected steps and can be repeated as indicated by step . If there is not another neighbor step can be performed.

In step it is determined whether there is another label in the selected node. For example the selected node can have multiple labels such as business Dogs as well as the Stocks label. If these additional labels have not been selected the inferred label generator can select one of the previously unselected labels and repeat steps . If all the labels in the node have been selected the label values of the selected node can be normalized as shown in step . For example the inferred label generator can normalize each label value so that it has a value between 0 and 1 where the label value s magnitude is proportional to its contribution relative to all the label values associated with that node.

In step it can be determined whether there are additional nodes in the graph to select. If there are additional nodes the method can return to step . If all the nodes in the graph have been selected the method can return to step to determine whether the specified number of iterations has been performed on the graph. If so the method can end.

In certain implementations after x iterations the inferred label generator can examine one or more of the nodes of the graph and probabilistically assign a label to each node based on the weights of the labels e.g. a label with the maximum label value can be assigned to the node .

In some implementations the number of the iterations is specified in advance. In other implementations the algorithm terminates when the label values for the labels at each node reach a steady state e.g. a state where the difference in the label value change between iterations is smaller than a specified epsilon .

In another alternative method label values for user nodes can be inferred by executing a random walk algorithm on the graphs. More specifically in some implementations given a graph G the inferred label generator can calculate label values or label weights for every node by starting a random walk from each node. The random walk algorithm can include reversing the direction of each edge in the graph if the edge is directed. If the edge is bi directional the edge can be left unchanged.

The inferred label generator can select a node of interest and start a random walk from that node to linked nodes. At each node where there are multiple out nodes e.g. nodes with links to multiple other nodes the inferred label generator can randomly select an edge to follow. If the edges are weighted the inferred label generator can select an edge based on the edge s weight e.g. the greatest weighted edge can be selected first .

If during the random walk a node is selected that is a labeling node e.g. used as a label for other nodes the classification for this walk is the label associated with the labeling node. The inferred label generator can maintain a tally of the labels associated with each classification.

If during the random walk a node is selected that is not a labeling node the inferred label generator selects the next random path or edge to follow.

The inferred label generator can repeat the random walk multiple times e.g. 1000 s to 100 000 s of times for each node of interest. After completion the inferred label generator can derive the label values based on the tally of labels. This process can be repeated for each node of interest.

The graph in is similar to the graph described in association with with the exception that label values expressed as circular nodes are assigned to each user node. The labels for the graph include Dogs Racing Stocks and Gambling.

After the first iteration as shown in Abby node has label values that include Dogs 1.0. Bob node has label values that include Racing 0.5 and Stocks 0.5. Ernie node has label values that include Stocks 0.5 and Gambling 0.5. During the initial iteration the inferred label generator assigns the label values to the user nodes based on the label values and . The remaining label values not mentioned for the nodes are null in this example.

Both Cal node and Dale node have label values and respectively that are null. This may occur if Cal and Dale s user profiles do not have any or enough information for use by the classifying module which assigns initial label values to the user nodes.

In certain implementations the label nodes such as and are not modified. Instead for every iteration the label nodes can provide one or more static label values that are used to modify the label values of user nodes to which they are linked.

In certain implementations one or more of the keyword nodes include terms that do not correspond with advertiser selected terms used to select online ads for display. For example the keyword recycled tofu associated with keyword node may not be an advertiser selected term also referred to as a non selected term . In some implementations the inferred label generator does not propagate label values related to this keyword throughout the nodes of the graph. Instead the non selected term is associated with advertiser selected terms. This process is described in more detail in association with .

The keywords nodes that include non selected terms such as the recycled tofu and NASCAR keyword nodes are not connected by edges to any of the Adgroup nodes . The keyword nodes that include non selected terms can be left in the graph in some implementations and may provide a source of inductive bias.

In the implementation illustrated in every node is directly or indirectly linked to a keyword selected by an advertiser. This may permit an advertiser to target a user regardless of whether the user is directly associated with an advertiser selected keyword. For example the Ernie node is associated with the NASCAR node by way of a membership relationship with the CannotDrive55 group node. The advertiser can target Ernie with ads associated with the keyword Racing because the NASCAR keyword node is mapped to the Racing keyword node as shown.

If a user such as a new member of the social network system does not have labels and joins the group as represented graphically by linking the user s node to the group s node an advertisement ad server can use keywords associated with the group node to advertise to the user. Additionally if the user is associated with keywords the inferred label generator can combine the current keywords with keywords that are associated with the group she joins.

As discussed previously the inferred label generator can use multiple graphs to determine the label values and compare the output of processed graphs to determine final label values based on expected an a priori distribution.

In certain implementations dummy nodes can be used in all of the graphs generated by the inferred label generator. In other implementations a user may specify for which graph s the dummy nodes may be used.

The system includes a processor a memory a storage device and an input output device . Each of the components and are interconnected using a system bus . The processor is capable of processing instructions for execution within the system . In one implementation the processor is a single threaded processor. In another implementation the processor is a multi threaded processor. The processor is capable of processing instructions stored in the memory or on the storage device to display graphical information for a user interface on the input output device .

The memory stores information within the system . In one implementation the memory is a computer readable medium. In one implementation the memory is a volatile memory unit. In another implementation the memory is a non volatile memory unit.

The storage device is capable of providing mass storage for the system . In one implementation the storage device is a computer readable medium. In various different implementations the storage device may be a floppy disk device a hard disk device an optical disk device or a tape device.

The input output device provides input output operations for the system . In one implementation the input output device includes a keyboard and or pointing device. In another implementation the input output device includes a display unit for displaying graphical user interfaces.

The features described can be implemented in digital electronic circuitry or in computer hardware firmware software or in combinations of them. The apparatus can be implemented in a computer program product tangibly embodied in an information carrier e.g. in a machine readable storage device or in a propagated signal for execution by a programmable processor and method steps can be performed by a programmable processor executing a program of instructions to perform functions of the described implementations by operating on input data and generating output. The described features can be implemented advantageously in one or more computer programs that are executable on a programmable system including at least one programmable processor coupled to receive data and instructions from and to transmit data and instructions to a data storage system at least one input device and at least one output device. A computer program is a set of instructions that can be used directly or indirectly in a computer to perform a certain activity or bring about a certain result. A computer program can be written in any form of programming language including compiled or interpreted languages and it can be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment.

Suitable processors for the execution of a program of instructions include by way of example both general and special purpose microprocessors and the sole processor or one of multiple processors of any kind of computer. Generally a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memories for storing instructions and data. Generally a computer will also include or be operatively coupled to communicate with one or more mass storage devices for storing data files such devices include magnetic disks such as internal hard disks and removable disks magneto optical disks and optical disks. Storage devices suitable for tangibly embodying computer program instructions and data include all forms of non volatile memory including by way of example semiconductor memory devices such as EPROM EEPROM and flash memory devices magnetic disks such as internal hard disks and removable disks magneto optical disks and CD ROM and DVD ROM disks. The processor and the memory can be supplemented by or incorporated in ASICs application specific integrated circuits .

To provide for interaction with a user the features can be implemented on a computer having a display device such as a CRT cathode ray tube or LCD liquid crystal display monitor for displaying information to the user and a keyboard and a pointing device such as a mouse or a trackball by which the user can provide input to the computer.

The features can be implemented in a computer system that includes a back end component such as a data server or that includes a middleware component such as an application server or an Internet server or that includes a front end component such as a client computer having a graphical user interface or an Internet browser or any combination of them. The components of the system can be connected by any form or medium of digital data communication such as a communication network. Examples of communication networks include a local area network LAN a wide area network WAN peer to peer networks having ad hoc or static members grid computing infrastructures and the Internet.

The computer system can include clients and servers. A client and server are generally remote from each other and typically interact through a network such as the described one. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other.

Although a few implementations have been described in detail above other modifications are possible. In certain implementations the label nodes do not have to be high level semantic nodes. Instead they may be individual keywords. For example they may be the union of the set of keywords that occur on every user s page description of themselves and every group s description etc.

In other implementations they may be keywords that are uncommon. For example the common word the may not provide much information but the relatively uncommon keyword basketball may. These types of words can be used by classifying module when implementing for example the common TF IDF term frequency inverse document frequency measure. Additionally the keywords can be selected from terms that advertisers often use to target online ads or keywords including hand selected terms that are of interest.

In certain implementations the graph or graphs that are created do not have to have an edge between all the friends that a user specifies. If the friends are ranked or weighted only the top N friends may be chosen e.g. a user s best friends . In other implementations the inferred label generator may only select the links for which each user specifies each other as a friend.

In yet other implementations a graph may have weighted edges. In some implementations each connection can be weighted the same. In other implementations edges can be weighted differently based on a number of factors. For example if user A specifies that he likes user B more than user C then the connection from A to B may be stronger than from A to C. In another example if user A and user B both specify that they like each other the strength of the connection may be stronger for example the connection may be twice as strong . In yet another example the inferred label generator can weight the edges in proportion to the out degree of a node.

Other weighting factors can be based on whether a user identifies with one group more than another group. In this case the weights on the edges may be greater for preferred groups such as groups that include frequent content postings from the user. Also a user may have a stronger affinity for some keywords rather than for other keywords e.g. a user may use the word venture much more than the word sports . Weights between users also can be based on a frequency of interaction such as messages visits to each other s sites etc. between the users.

In other implementations edges can be weighted based on when a friend was added. For example a link to a recently added friend can be weighted greater than previously added friends because the recently added friend may indicate a current interest. In another implementation an edge linking an older friend can be weighted more heavily. For example if the friend has been a friend for a while and it is known that the user takes off friends periodically then a friend who has lasted on the friend list can be weighted higher. Additionally similarity metrics between users e.g. a percent of overlap in the words that they use can be used to weight the connections. For example users that have a heavy overlap in term usage can have links that are weighted more heavily than users who do not have a significant overlap.

In some implementations the described algorithm and methods do not have to run until convergence. They can be stopped earlier and the inferred label generator can read the magnitudes of the label values at the nodes.

In some implementations the combination weights that are used to combine the end results may be set to 0 1 or 1 0 which selects only one of the graphs and not the other. Additionally the combination function may not be linear or continuous. For example the inferred label generator may specify that if the labels produced from graph A are too uniform then only labels generated from Graph B are selected for use for example in targeting online ads.

Additionally the term user as specified for example in the described pseudo code and method may actually be a group in the social network system. Additionally the term user can be substituted for other entities described in the graphs such as keywords advertisers Adgroups etc.

In certain implementations the set of labels used can include advertisements. For example the labels propagated through the graph can include advertisements on which one or more users have clicked. In this implementation after advertisement labels are inferred throughout a graph for each user the inferred label generator can output a set of advertisements on which each user may be likely to click.

In another implementation the inferred label generator can select labels for users to target ads etc. where the labels are derived based on an initial machine learning classification or the inferred label generator can use the inferred labels generated by executing the above algorithms and methods on the graph to infer labels for each user.

In certain implementations we can apply the same algorithms and methods based on a directed graph between users e.g. if user A says that user B is a friend this is a directed edge instead of an undirected one which implies that user B specifies that user A is also a friend .

Additionally in a data structure such as the graph of the links between users may be optional if there are enough common groups keywords. For example there may be enough common groups keywords if a defined threshold of the graph connectivity is met e.g. if the average user node is connected by two edges to at least two other users etc 

In certain implementations if there are more labeling nodes having label A than having label B the inferred label generator can normalize each label value so that the sum of the label values for each type is the same for each label e.g. across all labeling nodes the sum of label A 1.0 and across all labeling nodes the sum of label B 1.0 .

In other implementations the normalization step as described in the algorithms and methods linearly weights the magnitudes of the label values and scales them to 1.0 however the inferred label generator can use various normalization or non linear function for normalization.

Edges between keywords and advertisers can be added for example to show that an advertiser is only interested in a small set of keywords or has only one Adgroup etc.

Additionally click through rates can be incorporated into the algorithm. In some implementations a click through rate can include the number of times that an advertisement is either displayed to a viewer or selected by the viewer. The click through rate can be calculated based on advertisements displayed on other websites or solely on the social network. For example the click through rate can be calculated based on every time an advertisement is displayed on a search engines websites in response to search queries. This click through rate can be used to estimate the click through rate for the advertisement when used for the social network. In another example the click through rate can be calculated based on every time an advertisement is displayed is association with a website regardless of which web site displays the advertisement e.g. a system such as Google s AdSense which servers ads to display on websites can calculate a click through rate .

If the click through rate is determined by a separate system the inferred label generate can request this information when determining a weight to apply to an edge. In the previous examples described the connections between keywords and Adgroups or advertisers is shown unweighted. The connections may also be weighted to reflect click through rates that an advertiser Adgroup has with a particular keyword. For example the higher the click through rate the higher the weight. In other implementations only connections with a minimum click through rate or effective expected revenue derived for example from expected click through rate multiplied by the amount the advertiser has bid are modeled between keyword and Adgroup advertiser. Adgroups advertisers that do not have a minimum click through rate may be eliminated from the algorithms or methods used to determine labels. In still other implementations advertisement click through feedback can be used to tune the weights between edges based on a series of test iterations.

The inferred label generator can modify a graph s structure e.g. remove a percentage of the connections such as the connections that fall within the lowest one percent of weighted connections . This may approximate the random walk discussed previously. Additionally instead of maintaining all labels at every node the inferred label generator may only maintain subset of the labels.

In parallel implementations highly connected nodes may be processed using the same processor which may reduce message passing. Additionally the approximation can be computed in a layer wise manner where each layer includes for example a certain type of node. For example one layer may include user nodes another layer group nodes and another layer keyword nodes as shown in . In some implementations the algorithms and methods discussed above can be computed for pairs of layers instead of all the layers at once. For example some number of iterations of the method are computed between a first and second layer some between the second and third layer and some between the third and fourth layer and then the method can be repeated etc.

In addition the logic flows depicted in the figures do not require the particular order shown or sequential order to achieve desirable results. In addition other steps may be provided or steps may be eliminated from the described flows and other components may be added to or removed from the described systems. For example the first and second data stores can reside in a single storage device such as a hard drive.

