---

title: Any-image labeling engine
abstract: A system for tagging an image comprises a processor and a memory. The processor is configured to analyze an image associated with an image query using one or more computer vision analysis types to determine zero or more computer vision matches. Each computer vision match has one or more associated computer vision tags. In the event that it is determined that there are zero computer vision matches, the processor is further configured to analyze the image associated with the image query using a human vision analysis system to determine zero or more human vision matches. Each human vision match has one or more associated human vision tags. A memory coupled to the processor and configured to provide the processor with instructions.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09037600&OS=09037600&RS=09037600
owner: Yahoo! Inc.
number: 09037600
owner_city: Sunnyvale
owner_country: US
publication_date: 20110128
---
One of the most revolutionary technologies of the Internet age has been the search engine. Internet users instantly have access to a wealth of information by typing a few words e.g. keywords relevant to the thing they are interested in. The success of the text based search engine at providing a user with information in response to a text query and the proliferation of mobile cameras have created a demand for visual search where a user provides an image and is seeking information in response. Here the user may not know the words to describe his image or may find text entry inconvenient or impossible and thus traditional text search is not useful to him. He may desire to know what the image is of or to find information about an object in the image or where to get something like an object in the image e.g. procure or purchase the object in the image or what the name of a person in the image is or he may want some information extracted from the image e.g. the time on a clock the code from a barcode etc. . Image search has lagged text search because of the difficulties of computer vision processing. Recognizing images within a strict framework e.g. reading the time off of a clock is relatively easy however recognizing any object without restriction can be quite difficult.

The invention can be implemented in numerous ways including as a process an apparatus a system a composition of matter a computer program product embodied on a computer readable storage medium or server cluster and or a processor such as a processor configured to execute instructions stored on and or provided by a memory coupled to the processor. In this specification these implementations or any other form that the invention may take may be referred to as techniques. In general the order of the steps of disclosed processes may be altered within the scope of the invention. Unless stated otherwise a component such as a processor or a memory described as being configured to perform a task may be implemented as a general component that is temporarily configured to perform the task at a given time or a specific component that is manufactured to perform the task. As used herein the term processor refers to one or more devices circuits and or processing cores configured to process data such as computer program instructions.

A detailed description of one or more embodiments of the invention is provided below along with accompanying figures that illustrate the principles of the invention. The invention is described in connection with such embodiments but the invention is not limited to any embodiment. The scope of the invention is limited only by the claims and the invention encompasses numerous alternatives modifications and equivalents. Numerous specific details are set forth in the following description in order to provide a thorough understanding of the invention. These details are provided for the purpose of example and the invention may be practiced according to the claims without some or all of these specific details. For the purpose of clarity technical material that is known in the technical fields related to the invention has not been described in detail so that the invention is not unnecessarily obscured.

An any image labeling engine is disclosed. The any image labeling engine takes any image or video as input from a user and provides the user with a set of tags that describe the image or video content. In order to be able to process any image or video the any image labeling engine uses a hybrid approach with a computer vision module in conjunction with a human computation module. The any image labeling engine passes input images to one or more modules upon receipt. In the event that the computer vision module is able to identify the image and provide descriptive tags to the user the any image labeling engine is a success. The image is added to the computer vision database if not already present in order to broaden the spectrum of image searches the database is able to successfully respond to e.g. the image is added to the computer vision database where the image represents a new view of a previously existing object on the database . In the event that the computer vision module is not able to identify the image or in the event that the computer vision module is not able to provide as specific an answer as is desired the any image labeling engine waits for an answer from the human computation module. When the human computation module returns a set of tags for the image the tags are provided to the user. Additionally the image and tags are added to the computer vision database in order that the next time a similar image is received the computer vision database will be able to provide tags for the image query directly. The image recognition engine is able to learn e.g. the recognition engine is able to get smarter and faster with each visual query both from computer vision and human results.

In some embodiments the image recognition engine is able to programmatically monitor learned images and tags to identify those that have not been matched by subsequent queries these objects can then be deleted or removed from the primary computer vision database to ensure that the database size does not increase to such an extent that query response speed suffers. In various embodiments a computer vision match comprises one or more of the following a classification match an instance match an exact match a match based on a model of parts or any other appropriate computer vision method. In various embodiments tags comprise one or more of the following a noun an adjective an adverb a description of the image or a part of the image or any other appropriate type of tag. In some embodiments models are set up for search result items e.g. top image tagged for the day for example remote control or a new computer game related image are used for training a model to develop a new model for future image tagging.

In some embodiments human and computer modules are used to generate tags which are fed to a results processor. In some embodiments human feedback or tags are used to trigger the learning of the instance or classifier modules and databases.

In some embodiments a human module is queried after a computer module makes a preliminary assessment for example the computer module identifies a tag for an image e.g. a car and the tag is not specific enough or otherwise found to be inadequate e.g. based on context information so the image is provided to a specialist human tagger for more specific information e.g. a Toyota Prius a 1969 mustang convertible etc. . In various embodiments the context information comprises one or more of the following information from an originating web site e.g. car identifier object identification for the blind what is this flower information from an originating web service associated text information e.g. product information a specific user request e.g. what time does the clock say what is the tree name a specific request from a site submitting the image or metadata information from a service e.g. information about the submitting user geolocation etc. or any other appropriate context information.

In some embodiments a computer vision result provides a match with a probability of a computer vision match and the human vision is used to verify the computer vision match. For example computer vision system or module indicates that there is a probability of the image having a sailboat but it is not a strong match and human vision is used to confirm it. In other words there is not always 100 computer vision certainty and hints from computer vision are valuable because the computer vision module response is not completely adequate. The result is provided to the results processor and to a human tagger as additional information even though not a certainty and the resulting tags from human taggers and the original uncertain computer vision tags are analyzed using the results processor e.g. by confirmation of human tagger provided tags .

The computer computation module comprises a set of one or more computer vision engines for automatically identifying objects and scenes in images and video received by an image tagging system. In various embodiments the computer vision engines use an instance recognition module a classification module a 3D object recognition module an optical character recognition module a face detection identification or recognition module a barcode recognition module a color detection module or any other appropriate module. The instance recognition module identifies objects in images using a set of features describing the shapes and textures that are specific to the object. The classification module represents a visual class as a composition of parts which are then described by their shape and texture. These parts are allowed to move relative to one another to increase robustness to the typical variations in appearance of examples of a particular visual class. The optical character recognition module first detects whether there are characters in an image and then identifies which letters or digits these characters correspond to. The spatial arrangement of the characters is used to form words and numbers. The face detection identification or recognition module detects identifies or recognizes a face within an image and extracts facial features e.g. eyes nose chin jaw line ears mouse etc. and the relative configuration of these features forms a signature that is then matched with a database containing the face signatures of other individuals and or with a database of facial features. The barcode recognition module first detects 1 D and 2 D barcodes using coarse features that represent their distinctive shapes. Once the barcode is detected then the embedded code is recognized using fine grained analysis. If a code is of UPC or ISBN type the product information is looked up in a database. The clock recognition module uses the classification module to identify a clock in an image. Once a clock is detected then the module extracts features e.g. needles digits within the clock that determine the time. The color detection module builds a histogram of the colors that appear in the image. It then uses this histogram to extract the most dominant colors and describe them in a human or machine readable format e.g. mostly blue with some red .

In some embodiments a user captures a frame of a movie and the system identifies a frame number of the movie that corresponds to the captured image. In various embodiments movie information is identified from the image of the frame including one or more of the following a title a linked product actors in the scene a linked director s cut additional footage a web page or any other appropriate information.

The human computation module comprises a computer module that provides a number of human taggers with an image e.g. provides information so that the image is presented to a tagger on a computer display via a wired wireless and or cellular connection and receives a tag from the human taggers in response to the image presented e.g. receives a tag from a user as input to a computer interface for example a text string as input from a keyboard an audio snippet as input via a microphone etc. via a wired wireless and or cellular connection . In some embodiments the tags provided by a tagger to the human computation module for a particular image are stored until they are matched by tags received from one or more addition human tagger. In some embodiments the tags provided by a tagger to the human computation module for a particular image are compared to the tag results provided by the computer computation module to increase the confidence level in the correctness of the computer vision computation module s response. When a tag has been confirmed it is then delivered from the human computation module back to the any image labeling engine. Given a strong enough record of success a human tagger may be determined by the human computation module to be reliable without confirmation and have his tags delivered directly back from the human computation module to the any image labeling engine. The human computation engine may itself contain multiple human analysis modules and sources of human image identification including websites business process outsourcing firms crowdsourcing networks social network pages and gaming modules. Each human input can be statistically measured and monitored in terms of response.

The any image labeling engine additionally comprises a dashboard system for monitoring the system status. The dashboard system is able to provide a system management platform and information to a system administrator on image queries tags that have been applied to images system users image taggers and system status as well as allowing the administrator to train the computer vision module by adding images and tags at his discretion.

The garbage collection module comprises a set of one or more rules for identifying images that have been added to the computer vision database but which have not been matched against at least a predefined number of subsequent queries and during a defined period of time e.g. one match in 3 years 20 matches in one day etc. . Both the minimum number of matches required and the time period in which they must occur are parameters that can be adjusted to optimize performance. In various embodiments the garbage collection module deletes or archives the images that are identified by these parameters. In some embodiments the garbage collection module moves the images that are identified by these parameters to a different database with lower quality of service characteristics than the primary computer vision database. In various embodiments the garbage collection module is automated whereas in others it is executed manually or in any other appropriate manner. In various embodiments garbage collection of images includes images that have no features not enough features a blank image an ambiguous image an image with too many matches or any other appropriate images. In some embodiments the garbage collection module removes an object from a database. In some embodiments the object is removed from an instance database a model database a classification database a 3D database a barcode database an optical character database a face database a color database or any other appropriate database.

In some embodiments image objects and or classes are associated with a geographic location e.g. a global positioning location of a mobile phone so that subsequent queries can use the data. So for example if I take a picture of object X at location X we can do three things for subsequent query Y from location Y within a small radius around location X 

In the example shown computer vision engines comprises a set of one or more computer vision engines for automatically identifying objects in images or videos received by image tagging system . In various embodiments the computer vision engines use an instance recognition module a 3D recognition module a classification module an optical character recognition module a face identification module a barcode recognition module a color detection module or any other appropriate module. In some embodiments each different module utilized by computer vision engines is utilized by a separate computer vision engine. In some embodiments computer vision engines comprises more than one computer vision engine and the separate computer vision engines execute their modules in parallel. In some embodiments computer vision engines utilize information stored in object databases as part of executing computer vision modules. In some embodiments an instance recognition module utilizes object databases as its instance library. In some embodiments a classification module utilizes object databases as its class representation library.

In the example shown human computation module s comprises one or more human computation modules for image recognition. A human computation module for image recognition comprises one or more humans each capable of recognizing content in images and providing text tags for the images. In some embodiments a human computation module relays any tags provided by its humans directly to results processor . In some embodiments the human computation module additionally provides various processing functionality prior to providing the tags to results processor . In various embodiments processing functionality includes determining a status of a human tagger determining a validity of a tag based on a status of a human tagger confirming a validity of a tag based on receipt from multiple human taggers marking a tag with a validity status based on a status of a human tagger or any other appropriate processing functionality. In some embodiments human computation module s comprises more than one human computation module. In various embodiments human taggers are divided into separate rating categories or human computation modules based on specialty historical tag quality experience age location average time to provide results word quality tagger status or any other appropriate tagger classification. In some embodiments a human vision match is performed by providing one or more human taggers with an image query and context information where the human tagger provides one or more tags associated with the image query.

Results processor comprises a results processor for receiving and processing tags from computer vision engines and human computation module s . In the example shown results processor ranks the received tags according to a predetermined ranking algorithm and provides the highest ranked tag or tags to the image tagging system user e.g. the user that originally provided the image . In various embodiments the predetermined ranking algorithm utilizes a module ranking e.g. tags received from a computer vision instance module rank more highly than tags received from a computer vision classification module a module score e.g. tags received from the same module are ranked based on a score assigned to them by the module they are received from or any other appropriate ranking criteria.

In some embodiments results are combined from multiple modules to create a compound tag. For example a color module returns a tag red and a classification module returns a tag car with information that red and car originate from the same spatial location within the image and the results are combined to form a tag red car . In some embodiments union and intersection methods discussed with regard to human taggers are used with computer module tag results or a combination of human and computer vision tag results.

Learning module comprises a module for updating object databases based on the results of an image query. In some embodiments if an instance recognition module or a classification module are able to determine tags for an image based on information stored in object databases without finding an identical image stored in object databases the learning module then stores the image in object databases associated with the determined tags. In some embodiments the learning module determines whether adding the image to object databases will broaden the space of images associated with the determined tag. The learning module may then choose to store the image in object databases if it broadens the space of images associated with the tag. In various embodiments object databases comprises one or more of the following an instance image database a class database a model database a 3D object database an optical character database a face database a barcode database and a color database or any other appropriate database. In some embodiments if computer vision engines is not able to determine a tag associated with the image but human computation module s is learning module stores the image and associated tag in object databases so that the image can be automatically identified if it or a similar image is submitted in future queries. In some embodiments learning module processes an image before storing it and any associated tags in object databases . In some embodiments 2D images of objects at different views are combined for efficient 3D representation of an object. In various embodiments processing comprises background subtraction object geometry processing object class representation creation or any other appropriate processing.

Dashboard comprises an interface for an image tagging system administrator to manage visual queries and gain information about the current status of the image tagging system. In the example shown the dashboard system provides information to a system administrator on image queries tags that have been applied to images system users image taggers and system status as well as allowing the administrator to train the computer vision module by adding images and tags at his discretion.

Garbage collection module comprises a module to clean out one or more of the databases e.g. an instance database a classification database a model database that determines whether an instance of an image or a class or a model are no longer relevant for the database. In some embodiments an instance of an image is removed from the instance database in the event that the instance has not been accessed or returned a predetermined number of times in a predetermined period of time. In various embodiments a class or a model are removed from the relevant database in the event that the class or model are no longer relevant for the database.

In some embodiments modules or software of the image tagging system are executed using a processor. The processor is coupled to a memory configured to provide instructions to the processor. In various embodiments the processor comprises one two three or multiple processors.

Control then passes to and simultaneously or to and if no result or a partial or inadequate result is found e.g. based on context information tag s are not sufficient to adequately respond to the image query to . In the image is analyzed using computer vision engines e.g. computer vision engines of . In the image is analyzed using human computation modules e.g. human computation module s of . For example the human computation module provides a suitable number of human taggers with an image and receives tags from the human taggers. In some embodiments the tag s is are evaluated and or rated and provided to a results processor for combining. In various embodiments ratings of taggers unions of tags intersections of tags similarities of tags are used to determine a set of zero or more tags to provide to a results processor. In results received from the computer vision engines in and from the human computation modules in are combined. Image and video processing can be done with only computer vision with a combination of computer and human vision or with only human vision. In some embodiments the results are combined using a results processor e.g. results processor of .

In analysis is performed using the class recognition module. In some embodiments the class recognition module comprises one of the engines of computer vision engines of . In the example shown the class recognition module analyzes the image by comparing it with stored sets of images each set representing a particular class of objects e.g. trees cars fish weddings sunsets adult content etc. . The class recognition module is tuned to recognize images in a much more general way than the instance recognition module. The instance recognition module is only able to identify an image if there is an image in its database of the exact same thing e.g. it is a blue 2008 Toyota Prius but the class recognition module can recognize an image as being similar to a large set of other images even if there is no picture in the set of the exact same thing e.g. it is a car . In some embodiments the stored sets of images are stored in an object database e.g. an object database of object databases of . In the event that the class recognizer matches a class with a received image an associated set of tags e.g. car 4 wheeled vehicle etc. that are also stored in the class database are provided to a results combiner or selector. The one or more of the tags may be selected and provided in response to the image received to a user or submitter of the image to the tagging system.

In analysis is performed using side computer vision. In the example shown side computer vision comprises other more specialized computer vision modules. In some embodiments side computer vision comprises one or more of the engines of computer vision engines of . In various embodiments side computer vision comprises one or more of the following computer vision modules an optical character recognition module a face identification module a face recognition module a face detection module a 2D and 3D barcode recognition module a clock recognition module a color detection module or any other appropriate computer analysis module.

In some embodiments a feature and or a descriptor is a representation of visual space at a point in an image. In some embodiments features and or descriptors are concatenated to form a hyperfeature.

In some embodiments an image has a number of features and each feature of a given image is added as leaf node to a feature node by quantizing the feature. Each feature node representing a feature type in the inverted table has an associated weight based on how informative that feature type is. The more informative the feature type the greater its contribution to the match. Various metrics are used for the weighting of the feature type for example entropic weighting where commonly occurring feature types have lower weights because they are not distinctive. The weight associated with a feature type in the inverted table is a function of all the image feature types that are currently indexed and are stored in a separate weight table. The on line algorithm maintains a set of parameters that allows it to incrementally update the weight table as new images each with its own set of features are added to the database. In various embodiments the separate weight table is stored in memory and or in a parameter database. Without this online algorithm the computation of the weights would be too slow for real time learning as the inverted table would have to be recomputed from scratch using the entire dataset of feature weights. In some embodiments a short list of candidate image matches is determined by comparing the quantized descriptor with quantized descriptors for images stored in the database. The set of images whose quantized descriptor is closest e.g. least mean squared error 11 norm chi squared distance etc. to that of the image under analysis is selected as the short list of candidate image matches. In some embodiments the N images sharing the largest sets of quantized descriptors with the image under analysis are selected as the short list of candidate image matches where N is a predetermined desirable short list length. In some embodiments all images are selected that share at least M quantized descriptors with the image under analysis where M is a predetermined threshold.

In the sorted results list for the current query is retrieved. In some embodiments the results list is sorted by result score. In some embodiments sorting the results list by result score comprises sorting the results by the module that provided the result. In some embodiments in the event that multiple results were provided by the same module sorting the results list by module score comprises sorting multiple responses provided by the same module by module score. In some embodiments modules are ordered by level of specificity with most specific ranking the highest. In some embodiments a human computation module comprising experts in a particular area is considered to be the most specific. In some embodiments the computation modules are ordered from most to least specific human expert barcode recognition e.g. barcode recognition module performing analysis in of computer vision instance recognition e.g. instance recognition module performing analysis in of computer vision class recognition e.g. class recognition module performing analysis in of a generic human computation module a color detection module e.g. color detection module performing analysis in of . In it is determined whether the result is better than the previous best result. If the current query result is not better than the previous best result control passes to . In the current query result is saved in the database and the process ends. In some embodiments when the current query result is saved in the database it is added to the sorted results list for the current query in its appropriate place. If the current query result is determined to be better than the previous best result in the control passes to .

In the current query result is added to the top of the results list. In the set of results that have been determined in to be above the threshold are provided to the user. In the object databases e.g. object databases of are updated if necessary. In some embodiments the object databases are updated by a learning module e.g. learning module of . In some embodiments the current query result is provided to the learning module. In some embodiments the current image query is provided to the learning module. In some embodiments the learning module updates object databases with information related to the current image query and result. In image tagging modules are notified. In some embodiments image tagging modules are notified with the module and or score associated with the current query result. In some embodiments image tagging modules terminate execution upon notification of a query result. In some embodiments image tagging modules terminate execution upon notification of a query result from a more highly ranking module. In the current query result is saved in the database and the process ends. In some embodiments the updated results list with the new query at the top is saved in the database.

In it is determined whether to teach the class recognition module. In various embodiments the class recognition module is taught if the queried image is not present in the object database if the appropriate class is not present in the class database if the queried image is not associated with an appropriate class in the class database if the image presents a new exemplar of a class present in the class database or for any other appropriate reason. If it is determined in that the class recognition module should be taught control passes to . In the image is indexed in the class recognition module. In some embodiments indexing the image in the class recognition module comprises storing it in the object database. In some embodiments storing the image in the class recognition module comprises creating a new class in the class database and storing the image in the object database associated with that class. The process then ends. If it is determined in that the class recognition module should not be taught the process ends.

Although the foregoing embodiments have been described in some detail for purposes of clarity of understanding the invention is not limited to the details provided. There are many alternative ways of implementing the invention. The disclosed embodiments are illustrative and not restrictive.

