---

title: Method and system for performing event-matching with a graphical processing unit
abstract: A computer-implemented method for event matching in a complex event processing system includes receiving, with a computer processing device, a stream of event data; receiving, with a computer processing device, an event list and an access predicate list, wherein the event list includes one or more event data pairs; and identifying, with a graphical processing device, patterns in the stream of event data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09317456&OS=09317456&RS=09317456
owner: Infosys Technologies Ltd.
number: 09317456
owner_city: Bangalore
owner_country: IN
publication_date: 20110209
---
This application claims the benefit of Indian Patent Application No. 3054CHE 2010 filed Oct. 18 2010 which is hereby incorporated by reference in its entirety.

Complex Event Processing CEP is an event processing concept that deals with the task of processing multiple events i.e. an event cloud with the goal of identifying meaningful events within the event cloud. The event cloud may for example be an incoming stream of data received by an application and a CEP system may identify complex sequences of events in real time. CEP systems employ techniques such as detection of complex patterns of events event correlation and abstraction event hierarchies and relationships between events such as causality membership and timing and event driven processes. CEP systems may be used to discover information trends contained in the events happening across all layers in an organization and then analyze an event s impact from a macro level as a complex event. Thus action may be take in real time to respond to the complex event.

CEP is becoming crucial in enterprise scale applications. However processing the multitude of events that stream into an enterprise system within the least possible time is a daunting task. Current enterprise scale applications desire a rate of gigabytes per second GB sec . Algorithms and systems have aspired to achieve high scalability and high performance but such systems are complex and expensive. The main component of a CEP system is event matching for example utilizing a correlation engine CE which matches and identifies patterns in the incoming event stream. The CE has to process a large number of events per second and analyze different event processing strategies. Hence the CE has to provide high scalability availability and performance.

Systems and algorithms have been suggested in attempts to achieve high scalability and performance. For example Filtering Algorithms and Implementation for Very Fast Publish Subscribe Systems by F. Fabret et al. AMC SIGMOND 2001 the entire contents of which are incorporated herein by reference describes filtering algorithms and implementations for a CEP system. Parallel Event Processing for Content Based Publish Subscribe Systems by A. Farroukh et al. AMC DEBS 2009 the entire contents of which are incorporated herein by reference describes a parallel matching engine for a CEP system which leverages chip multi processors to increase throughput and reduce matching time. The Fabret and Farroukh articles teach similar two phase algorithms for pattern matching and Farroukh additionally teaches using event parallel processing.

The articles teach a two phase algorithm for pattern matching of events in a subscription system. According to the algorithm a predicate P an access predicate has to be matched before a set of subscriptions can be satisfied. An action can be taken on an event only if some condition is met. The algorithm defines a subscription as a set of predicates of the form A V R and an event is defined by a set of A V. Thus an event is said to match a predicate when A e A p and V e R V p . In the first phase of the algorithm the algorithm creates a bit vector to keep track of all predicates that are matched by an event and initializes the bit vector to 0. For every event attribute the algorithm hashes the attribute name to determine the table holding the attribute and the resulting table is accessed. Entries matched by this attribute are then indexed and corresponding bits in the bit vector are set to 1. In the second phase the list of access predicates is traversed and matched against the bit vector. When an access predicate matches a corresponding bit vector i.e. the corresponding bit vector position is set to 1 each set of methods is evaluated. If no match is found no method is processed.

In a CEP system maximum time and resources are consumed in the matching and processing engine that connects decoupled entities events. A CEP system can increase throughput of event processing by utilizing a chip multiprocessor s multiple cores or grid computing to handle these heavy computations in parallel. Still implementing CEP systems on conventional hardware such as chip multiprocessors using na ve multithreading is expensive and greater event matching speeds are desired.

According to embodiments a computer implemented method for event matching in a complex event processing system includes receiving with a computer processing device a stream of event data receiving with a computer processing device an event list and an access predicate list wherein the event list includes one or more event data pairs and identifying with a graphical processing device patterns in the stream of event data.

According to embodiments a computing device for implementing a complex event processing system includes a processing device executing instructions and a graphical processing device configured to receive instructions and a stream of event data from the processing device and to identify patterns in the stream of event data.

According to embodiments computer readable instructions are stored on a non transitory storage device the computer readable instructions configured to be processed by a processor to instruct the processor to perform a method including receiving with a computer processing device a stream of event data receiving with a computer processing device an event list and an access predicate list wherein the event list comprises one or more event data pairs and identifying with a graphical processing device patterns in the stream of event data.

While the method and system for performing event matching with a graphical processing unit is described herein by way of example and embodiments those skilled in the art will recognize that the method and system is not limited to the embodiments or drawings described. It should be understood that the drawings and description are not intended to limit embodiments to the particular form disclosed. Rather the intention is to cover all modifications equivalents and alternatives falling within the spirit and scope of the invention defined by the appended claims. Any headings used herein are for organizational purposes only and are not meant to limit the scope of the description or the claims. As used herein the word may is used in a permissive sense i.e. meaning having the potential to rather than the mandatory sense i.e. meaning must . Similarly the words include including and includes mean including but not limited to.

Embodiments of a CEP method and system perform event matching utilizing graphical processing unit GPU hardware. In this fashion an event matching algorithm may provide a higher throughput of event processing than those utilizing conventional chip multi processors. Owing to a high computation to communication ratio utilizing a GPU for an event matching algorithm enables a CEP system to quickly process large numbers of incoming events for high throughput. Additionally implementing a GPU for an event matching algorithm is far more economical than multi core processing or grid computing.

GPUs were developed for conventional computing devices for heavy and complex data processing required in the graphics field. GPUs have evolved into highly parallel multithreaded many core processors with tremendous computational power and very high memory bandwidth. In recent years GPUs have been recognized as incredible resources for both graphics and non graphics processing. GPUs are especially well equipped to address problems that can be expressed as data parallel computations i.e. the same program is executed on many data elements in parallel with high arithmetic intensity i.e. the ratio of arithmetic operations to memory operations . Because the same program is executed for each data element with different data there is a lower requirement for sophisticated flow control than with conventional chip multi processors. Additionally because each program is executed on many data elements and has high arithmetic intensity the memory access latency can be hidden with calculations instead of big data caches as required by conventional chip multi processors.

Event matching in a CEP system may be implemented with software executed on a computing device of . Computing device has one or more processors such as central processing unit CPU designed to process instructions for example computer readable instructions stored on a storage device . By processing instructions CPU transforms underlying data to process instructions for example to instruct a GPU to perform event matching in a CEP system. GPU may be any GPU device that supports general purpose computing. Storage device may be any type of storage device e.g. an optical storage device a magnetic storage device a solid state storage device etc. . Alternatively instructions may be stored in remote storage devices for example storage devices accessed over a network e.g. the Internet . Computing device additionally has memory and an input controller . A bus operatively couples components of computing device including processor memory storage device input controller GPU and any other devices e.g. network controllers sound controllers etc. . GPU is operatively coupled e.g. via a wired or wireless connection to a display device e.g. a monitor television mobile device screen etc. in such a fashion that display controller can transform the display on display device e.g. in response to modules executed . Input controller is operatively coupled e.g. via a wired or wireless connection to input device e.g. mouse keyboard touch pad scroll ball etc. in such a fashion that input can be received from a user. Computing device display device and input device may be separate devices e.g. a personal computer connected by wires to a monitor and mouse may be integrated in a single device e.g. a mobile device with a touch screen having a mobile GPU or any combination of devices e.g. a computing device operatively coupled to a touch screen display device a plurality of computing devices attached to a single display device and input device etc. .

GPUs typically handle computations only for computer graphics. General purpose computing on GPUs however is the technique of using a GPU to perform computations in an application traditionally handled by a CPU. The highly parallel hardware of a GPU enables a GPU to efficiently compute complex algorithms. Initially to perform general purpose computing on a GPU computations were required to be mapped to graphics application programming interfaces APIs . GPU architectures for example the Compute Unified Device Architecture CUDA developed by NVIDIA for use with NVIDIA GPUs are hardware and software architectures for issuing and managing computations on the GPU as a data parallel computing device without the need of mapping computations to a graphics API. GPGPU architectures provide extensions for high level programming languages e.g. C to allow applications to directly utilize the GPU without graphics APIs. Alternative architectures APIs and programming languages may be supported for example OpenCL is an open standard supported by many companies.

Referring again to for general purpose computing on GPU GPU runs as a co processor to CPU . GPU may be referred to as a device while CPU may be referred to as a host . Part of an application that can be run many times on independent data can be executed by GPU via many parallel threads that achieve high throughput. For example shows a conceptual illustration of a conventional CPU having many transistors devoted to data caching e.g. cache and flow control e.g. control device and less transistors devoted to data processing i.e. few arithmetic logic units ALUs . In contrast shows a conceptual illustration of a GPU which is designed for highly parallel computation and has more transistors devoted to data processing rather than data caching and flow control. GPU provides for many parallel streams with multiple control devices and caches and many ALUs operatively coupled e.g. via high bandwidth bus lines which are not shown to DRAM . For example a GPU made by NVIDIA may have 240 cores on a single chip and greater than 100 GB sec bandwidth between the chip and the graphics memory. Such a GPU may process event matching at 50 100 times the speed of a single core CPU. Such a GPU may even process event matching at 10 times the speed of two quad core INTEL XEON processors. GPUs additionally may provide about 10 times the improvement according to a price versus processing performance ratio and are energy efficient.

The component of the program that is off loaded from CPU onto GPU is the kernel. The kernel is the portion of an application that is executed many times but independently on different data. The kernel can be isolated into a function that is executed in parallel on GPU as many different threads. To that effect such a function is compiled to the instruction set of CPU and the resulting program the kernel is downloaded to GPU . Both CPU and GPU may maintain their own DRAM. Data may then be copied from one DRAM to the other through optimized API calls that utilize a high performance Direct Memory Access DMA engine of GPU .

Referring to the batch of threads such as threads that execute kernel is organized as a grid of thread blocks such as thread blocks . The threads e.g. threads making up each block e.g. block cooperate together by efficiently sharing data through some fast shared memory e.g. shared memory shown in . Threads may synchronize their execution to coordinate memory access. Kernel may specify synchronization points thereby suspending selected ones of threads until each thread reaches a synchronization point.

Each thread is associated with a unique thread ID local to a block. An application may specify a block as a two or three dimensional array of arbitrary size and identify each thread using a two or three component index. For a two dimensional block of size D D the thread ID of a thread of index x y may be x y D and for a three dimensional array the thread ID of a thread of index x y z may be x y D z DD .

Each block e.g. block may contain a limited number of threads depending on the hardware. Blocks having the same dimensions i.e. the same number of threads may be batched together to form a grid of blocks. As shown in blocks having the same dimensions i.e. the same number of threads are batched together to form grid . In like fashion blocks of a second dimension are batched together to form grid . By batching blocks of the same dimensions together the total number of threads that can be launched in a single kernel invocation is increased. Thus threads within the same grid may communicate and synchronize with each other.

Embodiments of methods and systems for implementing event matching on a GPU may be selectively chosen depending on a number of events to be processed.

Embodiments may process multiple events per thread i.e. inter task parallelism thereby providing a high throughput for processing many parallel events. By allowing threads to work independently on separate events the total matching time and throughput are increased. Such embodiments may be implemented in systems receiving event data at a rate of GB sec. In these embodiments each thread in GPU may be assigned a group of events and execute an event matching algorithm. In a first phase of this embodiment every thread e.g. thread may pick up an event and compute a bit vector and store it in the thread s local memory e.g. local memory . For every event attribute the algorithm may hash the attribute name to determine the table holding the attribute and the resulting table may be accessed. Entries of the table matched by the attribute may then be indexed and corresponding bits in the bit vector may be set to 1. In a second phase of these embodiments the thread traverses an access predicate list and each access predicate is evaluated against the bit vector. The thread then saves the matches. When an access predicate matches a corresponding bit vector i.e. the corresponding bit vector position is set to 1 each set of methods is evaluated.

Step 2 For each thread load one Event Data E A V pair and the access predicate list in the thread s local memory.

Step 3 Initialize a bit vector the size of the number of predicates in the access predicate list Number of Predicates N to 0 in the thread s local memory.

Phase 2 Traverse the access predicate list and for each access predicate compute the final result of a Boolean function e.g. a comparative function whether a match is found.

Other embodiments may process a single event per thread or block of threads i.e. intra task parallelism . Thus by multiple threads working collaboratively on a single event the processing time for that event may be reduced. In these embodiments each thread in GPU may be assigned a group of predicates and execute an event matching algorithm to evaluate matches with the event. Each thread may set corresponding bits in a bit vector stored in the thread s local memory e.g. local memory to 1 to indicate a match. After all threads have executed the event matching algorithm the bit vectors in each thread s local memory are merged into a global bit vector stored in a GPU s global memory e.g. global memory . The access predicate list is then split into chunks across the threads and each thread evaluates the chunk of access predicates against the bit vector and saves the matches.

Step 2 Allocate either a warp of threads 16 threads or a block of threads e.g. a number of threads 512 per event.

Step 3 Load the Event Data E A V and access predicate list in the warp or block of thread s shared memory e.g. shared memory or in the GPU s global memory e.g. global memory and initialize a bit vector in the warp or block of threads to 0.

Step 4 In each thread in the warp or block of threads receive a group of predicates and compute the result of a Boolean function to determine whether a match is found and set corresponding partial bit vectors in the thread s local memory e.g. local memory .

Step 5 Synchronize threads until all remaining threads fill their respective partial bit vectors. Upon synchronization merge each thread s partial bit vector to form a global bit vector in global memory e.g. memory .

Step 6 Split the access predicate into chunks across different threads and compute for each thread whether a match is found.

Of course while multiple embodiments are disclosed one of ordinary skill in the art understands that variations or combinations of these embodiments may be implemented as well. For example a hybrid technique may combine the embodiments to provide flexibility of reducing the matching time of a single event or increasing overall throughput.

According to embodiments grid dimensions for example dimensions of grid shown in may be selected to substantially utilize maximum GPU resources at all times i.e. to keep the GPU busy . For example for a grid having dimensions X blocks long and Y blocks wide X and Y may be chosen both so that X Y can handle the average number of events entering the CEP system but also to utilizes substantially all or at least a significant amount of GPU resources. For example in a CEP system receiving an average number of events N the number of blocks in a grid may be chosen to satisfy the relation X Y N.

While this disclosure generally refers to a GPU having DRAM one of ordinary skill in the art understands that the GPU and DRAM may not be integrated within a single chip. One or more memory chips may be physically separate from the GPU and operatively coupled thereto for example via a high bandwidth bus line.

The invention has been described through embodiments. However various modifications can be made without departing from the scope of the invention as defined by the appended claims and legal equivalents.

